{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to MIRIX","text":"<p>MIRIX is a memory system for agents. It captures, structures, and retrieves memories so your agents can stay consistent over time.</p> Important Update: 0.1.6 (Main) vs 0.1.3 (Desktop Agent)       Starting with <code>0.1.6</code>, the <code>main</code> branch is a brand-new release line where Mirix is a pure memory system that can be plugged into any existing agents. The desktop personal assistant (frontend + backend) has been deprecated and is no longer shipped on <code>main</code>. If you need the earlier desktop application with the built-in agent, use the <code>desktop-agent</code> branch.      <ul> <li> <p> Get Started</p> <p>Overview and a fast path to your first memory write.</p> <p> Overview</p> </li> <li> <p> Memory Write</p> <p>Configure LLMs, embeddings, and retention policies.</p> <p> Configuration</p> </li> <li> <p> Memory Search</p> <p>Query memories with keyword and embedding search.</p> <p> Search</p> </li> <li> <p> Contributing</p> <p>Learn how to contribute to the MIRIX project and join our community.</p> <p> Contribute</p> </li> </ul>"},{"location":"#how-mirix-works","title":"How MIRIX Works","text":"<pre><code>flowchart TD\n    A[Inputs] --&gt; B[Meta Agent]\n    B --&gt; C{Content Analysis}\n    C --&gt; D[Core Memory&lt;br/&gt;Personal Info]\n    C --&gt; E[Episodic Memory&lt;br/&gt;Activities]\n    C --&gt; F[Semantic Memory&lt;br/&gt;Knowledge]\n    C --&gt; G[Procedural Memory&lt;br/&gt;Workflows]\n    C --&gt; H[Resource Memory&lt;br/&gt;Documents]\n    C --&gt; I[Knowledge Vault&lt;br/&gt;Credentials]\n\n    J[Agent Query] --&gt; K[Retrieval]\n    K --&gt; L[Memory Search]\n    D --&gt; L\n    E --&gt; L\n    F --&gt; L\n    G --&gt; L\n    H --&gt; L\n    I --&gt; L\n    L --&gt; M[Intelligent Response]</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":"<p>Agent Memory</p> <p>Persist key facts and decisions so agents remain consistent across sessions.</p> <p>Retrieval and Recall</p> <p>Query episodic and semantic memory with keyword or embedding search.</p> <p>Structured Knowledge</p> <p>Store procedures, resources, and core facts in dedicated memory types.</p> <p>Multi-Agent Systems</p> <p>Plug MIRIX into different agents without rebuilding memory pipelines.</p>"},{"location":"#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.11 or later</li> <li>PostgreSQL 17 (recommended) or SQLite</li> <li>An API key for your LLM provider</li> </ul> <p>Ready to add memory to your agents?</p> <p>Get Started Now</p>"},{"location":"#key-capabilities","title":"Key Capabilities","text":""},{"location":"#memory-system","title":"Memory System","text":"<ul> <li>Six memory components with dedicated agents</li> <li>Configurable retention and decay policies</li> <li>Structured writes from conversation input</li> </ul>"},{"location":"#multi-agent-architecture","title":"Multi-Agent Architecture","text":"<ul> <li>8 specialized agents working collaboratively</li> <li>6 memory components for organized data storage</li> <li>Coordinated workflow for efficient processing</li> </ul>"},{"location":"#advanced-search","title":"Advanced Search","text":"<ul> <li>PostgreSQL-native BM25 search</li> <li>Vector similarity search using embeddings</li> <li>Field-specific search across all memory types</li> </ul>"},{"location":"#privacy-security","title":"Privacy &amp; Security","text":"<ul> <li>All long-term data stored locally</li> <li>User-controlled privacy settings</li> <li>Enterprise-grade PostgreSQL security</li> </ul>"},{"location":"contributing/","title":"Contributing to MIRIX","text":"<p>Thank you for your interest in contributing to MIRIX! Here's how you can help improve our multi-agent personal assistant project.</p>"},{"location":"contributing/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<p>Found a bug? Please open a GitHub Issue with:</p> <ul> <li>Clear description of the problem</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Your environment details (OS, Python version, etc.)</li> </ul>"},{"location":"contributing/#feature-requests","title":"\ud83d\udca1 Feature Requests","text":"<p>Have an idea for a new feature? We'd love to hear it! Please:</p> <ol> <li>Check our GitHub Discussions to see if it's already been suggested</li> <li>Open a new discussion in the \"Ideas\" category to get community feedback</li> <li>If there's interest, we can create a GitHub issue to track development</li> </ol>"},{"location":"contributing/#code-contributions","title":"\ud83d\udd27 Code Contributions","text":"<p>Want to contribute code? Here's the simple process:</p> <ol> <li>Fork the repository on GitHub</li> <li>Create a new branch for your changes:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> <li>Make your changes and commit them</li> <li>Push to your fork and create a pull request</li> <li>Describe your changes clearly in the PR description</li> </ol>"},{"location":"contributing/#getting-help","title":"\ud83d\udcac Getting Help","text":"<p>Need help or have questions?</p> <ul> <li>General questions: Start a GitHub Discussion</li> <li>Direct contact: Email us at yuw164@ucsd.edu</li> </ul>"},{"location":"contributing/#community-guidelines","title":"\ud83e\udd1d Community Guidelines","text":"<ul> <li>Be respectful and constructive</li> <li>Help others when you can</li> <li>Keep discussions focused and relevant</li> </ul> <p>Thanks for contributing to MIRIX! \ud83d\ude80 </p>"},{"location":"architecture/memory-components/","title":"Memory Components","text":"<p>MIRIX organizes information into six distinct memory components, each designed to handle specific types of data and provide optimal retrieval performance.</p>"},{"location":"architecture/memory-components/#1-core-memory","title":"1. Core Memory","text":"<p>Purpose: Persistent information that should always be visible to the agent when interacting with the user.</p> <p>Inspired by: MemGPT's core memory architecture</p>"},{"location":"architecture/memory-components/#structure","title":"Structure","text":"<p>Core Memory is organized in multiple blocks (<code>human</code> and <code>persona</code>)</p> <pre><code>&lt;human 117/500 characters&gt;\nUser's name is David\nUser prefers coffee over tea\nUser works as a software engineer\nUser enjoys reading sci-fi novels\n&lt;/human&gt;\n\n&lt;persona 24/5000 characters&gt;\nI am a helpful assistant\n&lt;/persona&gt;\n</code></pre>"},{"location":"architecture/memory-components/#2-episodic-memory","title":"2. Episodic Memory","text":"<p>Purpose: Captures context-specific events and temporal activities, serving as a summarization or calendar of user behaviors.</p>"},{"location":"architecture/memory-components/#structure_1","title":"Structure","text":"<p>Each episodic entry contains:</p> <pre><code>{\n  \"event_type\": \"user_message\",\n  \"summary\": \"User reviewed quarterly sales report\",\n  \"details\": \"Detailed analysis of Q3 sales performance, identified key growth areas in mobile segment, discussed strategies with marketing team\",\n  \"actor\": \"user\",\n  \"timestamp\": \"2025-03-05 10:15\"\n}\n</code></pre>"},{"location":"architecture/memory-components/#3-semantic-memory","title":"3. Semantic Memory","text":"<p>Purpose: Maintains general knowledge, concepts, and abstracted information independent of temporal context.</p>"},{"location":"architecture/memory-components/#structure_2","title":"Structure","text":"<p>Each semantic entry includes:</p> <pre><code>{\n  \"name\": \"PostgreSQL\",\n  \"summary\": \"Open-source relational database management system\",\n  \"details\": \"Powerful, enterprise-grade database with advanced features like JSONB support, full-text search, and vector extensions. Preferred by user for its performance and reliability.\",\n  \"source\": \"user_interaction\"\n}\n</code></pre>"},{"location":"architecture/memory-components/#content-types","title":"Content Types","text":"<ul> <li>Factual Knowledge: \"Harry Potter is written by J.K. Rowling\"</li> <li>Facts and Understandings about Other People: \"John is user's good friend who likes jogging\"</li> <li>Concepts: \"Machine learning algorithms and their applications\"</li> </ul>"},{"location":"architecture/memory-components/#example-entries","title":"Example Entries","text":"<pre><code>[\n  {\n    \"name\": \"MkDocs Material\",\n    \"summary\": \"Documentation framework based on MkDocs\",\n    \"details\": \"Static site generator that creates beautiful documentation sites from Markdown files. Features include responsive design, search functionality, and extensive customization options.\",\n    \"source\": \"documentation_project\"\n  },\n  {\n    \"name\": \"Team Standup Meeting\",\n    \"summary\": \"Daily team synchronization meeting\",\n    \"details\": \"Occurs every weekday at 9 AM, attended by development team to discuss progress, blockers, and daily goals. Usually lasts 15-20 minutes.\",\n    \"source\": \"recurring_activity\"\n  }\n]\n</code></pre>"},{"location":"architecture/memory-components/#4-procedural-memory","title":"4. Procedural Memory","text":"<p>Purpose: Records process workflows and step-by-step instructions for accomplishing specific tasks.</p>"},{"location":"architecture/memory-components/#structure_3","title":"Structure","text":"<p>Each procedural entry contains:</p> <pre><code>{\n  \"entry_type\": \"workflow\",\n  \"description\": \"Deploy application to production\",\n  \"steps\": [\n    \"1. Run test suite to ensure all tests pass\",\n    \"2. Create production build with 'npm run build'\",\n    \"3. Review build artifacts for any issues\",\n    \"4. Deploy to staging environment first\",\n    \"5. Perform smoke tests on staging\",\n    \"6. Deploy to production using CI/CD pipeline\",\n    \"7. Monitor application metrics post-deployment\"\n  ]\n}\n</code></pre>"},{"location":"architecture/memory-components/#example-entries_1","title":"Example Entries","text":"<pre><code>[\n  {\n    \"entry_type\": \"workflow\",\n    \"description\": \"Setting up new development environment\",\n    \"steps\": [\n      \"1. Install Python 3.11 or later\",\n      \"2. Set up virtual environment with 'python -m venv venv'\",\n      \"3. Activate virtual environment\",\n      \"4. Install dependencies with 'pip install -r requirements.txt'\",\n      \"5. Configure environment variables in .env file\",\n      \"6. Initialize database with 'python manage.py migrate'\",\n      \"7. Run development server with 'python manage.py runserver'\"\n    ]\n  },\n  {\n    \"entry_type\": \"guide\",\n    \"description\": \"Troubleshooting PostgreSQL connection issues\",\n    \"steps\": [\n      \"1. Check if PostgreSQL service is running\",\n      \"2. Verify database exists with 'psql -l'\",\n      \"3. Test connection with 'psql -U username -d database'\",\n      \"4. Check firewall settings if connecting remotely\",\n      \"5. Verify authentication configuration in pg_hba.conf\"\n    ]\n  }\n]\n</code></pre>"},{"location":"architecture/memory-components/#5-resource-memory","title":"5. Resource Memory","text":"<p>Purpose: Manages active documents and project-related files that the user interacts with.</p>"},{"location":"architecture/memory-components/#structure_4","title":"Structure","text":"<p>Each resource entry includes:</p> <pre><code>{\n  \"title\": \"Project Proposal - Q4 2024\",\n  \"summary\": \"Comprehensive proposal for new mobile application development project including timeline, budget, and technical specifications\",\n  \"resource_type\": \"pdf_text\",\n  \"content\": \"# Project Proposal\\n\\n## Executive Summary\\nThis proposal outlines the development of a new mobile application...\\n\\n## Technical Requirements\\n- React Native framework\\n- PostgreSQL database\\n- AWS cloud infrastructure...\"\n}\n</code></pre>"},{"location":"architecture/memory-components/#example-entries_2","title":"Example Entries","text":"<pre><code>[\n  {\n    \"title\": \"API Documentation Draft\",\n    \"summary\": \"Initial draft of REST API documentation for the customer management system, includes endpoint specifications and example requests\",\n    \"resource_type\": \"markdown\",\n    \"content\": \"# Customer Management API\\n\\n## Overview\\nThis API provides endpoints for managing customer data...\\n\\n## Endpoints\\n\\n### GET /api/customers\\nRetrieve list of customers...\"\n  },\n  {\n    \"title\": \"Meeting Recording - Sprint Planning\",\n    \"summary\": \"Voice recording from sprint planning meeting discussing user stories and development priorities for next iteration\",\n    \"resource_type\": \"voice_transcript\",\n    \"content\": \"Transcript: 'Let's start with the user authentication story. Based on our previous discussion, we need to implement OAuth 2.0 integration...'\"\n  }\n]\n</code></pre>"},{"location":"architecture/memory-components/#6-knowledge-vault","title":"6. Knowledge Vault","text":"<p>Purpose: Securely stores structured personal data such as addresses, phone numbers, contacts, and credentials.</p>"},{"location":"architecture/memory-components/#structure_5","title":"Structure","text":"<p>Each vault entry contains:</p> <pre><code>{\n  \"entry_type\": \"credential\",\n  \"source\": \"github\",\n  \"sensitivity\": \"high\",\n  \"secret_value\": \"ghp_xxxxxxxxxxxxxxxxxxxx\",\n  \"caption\": \"GitHub Personal Access Token for API access\"\n}\n</code></pre>"},{"location":"architecture/memory-components/#sensitivity-levels","title":"Sensitivity Levels","text":"<ul> <li><code>low</code>: General bookmarks and public information</li> <li><code>medium</code>: Contact information and non-critical data</li> <li><code>high</code>: Passwords, API keys, and sensitive credentials</li> </ul>"},{"location":"architecture/memory-components/#security-features","title":"Security Features","text":"<ul> <li>Encryption: Sensitive data encrypted at rest</li> <li>Access Control: Restricted access based on sensitivity level</li> <li>Audit Trail: All access to sensitive data is logged</li> <li>Automatic Expiration: Credentials can have expiration dates</li> </ul>"},{"location":"architecture/memory-components/#example-entries_3","title":"Example Entries","text":"<pre><code>[\n  {\n    \"entry_type\": \"api_key\",\n    \"source\": \"openai\",\n    \"sensitivity\": \"high\",\n    \"secret_value\": \"sk-proj-xxxxxxxxxxxxxxxxxxxx\",\n    \"caption\": \"OpenAI API key for ChatGPT integration\"\n  },\n  {\n    \"entry_type\": \"bookmark\",\n    \"source\": \"user_provided\",\n    \"sensitivity\": \"low\",\n    \"secret_value\": \"https://docs.mirix.ai/\",\n    \"caption\": \"MIRIX documentation website\"\n  },\n  {\n    \"entry_type\": \"contact_info\",\n    \"source\": \"user_profile\",\n    \"sensitivity\": \"medium\",\n    \"secret_value\": \"john.doe@example.com\",\n    \"caption\": \"Primary email address\"\n  }\n]\n</code></pre>"},{"location":"architecture/memory-components/#memory-interaction-patterns","title":"Memory Interaction Patterns","text":""},{"location":"architecture/memory-components/#search-integration","title":"Search Integration","text":"<p>All memory components support unified search:</p> <pre><code># Search across all memory types\nresults = search_in_memory(\n    query=\"machine learning project\",\n    memory_type='episodic', # can be chosen from ['all', 'episodic', 'semantic', 'resource', 'procedural', 'knowledge_vault']\n    limit=20\n)\n</code></pre>"},{"location":"architecture/memory-components/#memory-optimization","title":"Memory Optimization","text":""},{"location":"architecture/memory-components/#automatic-cleanup","title":"Automatic Cleanup","text":"<ul> <li>Core Memory: Rewrites blocks when approaching capacity</li> <li>Episodic Memory: Archives old entries based on relevance</li> <li>Semantic Memory: Merges duplicate concepts</li> <li>Procedural Memory: Updates workflows based on usage patterns</li> <li>Resource Memory: Compresses or removes unused resources</li> <li>Knowledge Vault: Expires outdated credentials</li> </ul>"},{"location":"architecture/memory-components/#whats-next","title":"What's Next?","text":"<p>Learn about MIRIX's advanced search capabilities:</p> <p>Search Capabilities \u2192 </p>"},{"location":"architecture/multi-agent-system/","title":"Multi-Agent System","text":"<p>MIRIX consists of eight specialized agents that work collaboratively to process your digital activities and manage memory efficiently.</p>"},{"location":"architecture/multi-agent-system/#agent-overview","title":"Agent Overview","text":"<pre><code>graph TB\n    subgraph \"Input Processing\"\n        A[User Input] --&gt; B[Meta Agent]\n    end\n\n    subgraph \"Memory Managers\"\n        C[Core Memory Manager]\n        D[Episodic Memory Manager]\n        E[Semantic Memory Manager]\n        F[Procedural Memory Manager]\n        G[Resource Memory Manager]\n        H[Knowledge Vault Manager]\n    end\n\n    subgraph \"User Interaction\"\n        I[Chat Agent]\n    end\n\n    subgraph \"Memory Base\"\n        J[(Shared Memory Database)]\n    end\n\n    B --&gt; C\n    B --&gt; D\n    B --&gt; E\n    B --&gt; F\n    B --&gt; G\n    B --&gt; H\n\n    C --&gt; J\n    D --&gt; J\n    E --&gt; J\n    F --&gt; J\n    G --&gt; J\n    H --&gt; J\n\n    I --&gt; J\n    J --&gt; I</code></pre>"},{"location":"architecture/multi-agent-system/#agent-roles","title":"Agent Roles","text":""},{"location":"architecture/multi-agent-system/#meta-agent","title":"Meta Agent","text":"<p>Role: Central coordinator and content analyzer</p> <p>Responsibilities:</p> <ul> <li>Analyzes incoming user content (text, images, voice)</li> <li>Determines which memory components need updates</li> <li>Routes the input content to relevant Memory Managers</li> </ul> <p>Workflow: <pre><code># Pseudo-code for Meta Agent processing\ndef trigger_memory_update(self: \"Agent\", user_message: object, memory_types: List[str]) -&gt; Optional[str]:\n    \"\"\"\n    Choose which memory to update. This function will trigger another memory agent which is specifically in charge of handling the corresponding memory to update its memory. Trigger all necessary memory updates at once. \n\n    Args:\n        memory_types (List[str]): The types of memory to update. It should be chosen from the following: \"core\", \"episodic\", \"resource\", \"procedural\", \"knowledge_vault\", \"semantic\". For instance, ['episodic', 'resource'].\n    \"\"\"\n\n    # ... and so on\n</code></pre></p>"},{"location":"architecture/multi-agent-system/#chat-agent","title":"Chat Agent","text":"<p>Role: Natural language conversation interface</p> <p>Responsibilities: - Handles user queries and conversations - Searches across all memory components using <code>search_in_memory()</code> - Synthesizes retrieved information into contextual responses - Maintains conversation flow and context</p> <p>Search Process: <pre><code>def search_in_memory(self: \"Agent\", memory_type: str, query: str, search_field: str, search_method: str, timezone_str: str) -&gt; Optional[str]:\n    \"\"\"\n    Choose which memory to search. All memory types support multiple search methods with different performance characteristics. Most of the time, you should use search over 'details' for episodic memory and semantic memory, 'content' for resource memory (but for resource memory, `embedding` is not supported for content field so you have to use other search methods), 'description' for procedural memory. This is because these fields have the richest information and is more likely to contain the keywords/query. You can always start from a thorough search over the whole memory by setting memory_type as 'all' and search_field as 'null', and then narrow down to specific fields and specific memories.\n\n    Args:\n        memory_type: The type of memory to search in. It should be chosen from the following: \"episodic\", \"resource\", \"procedural\", \"knowledge_vault\", \"semantic\", \"all\". Here \"all\" means searching in all the memories. \n        query: The keywords/query used to search in the memory.        \n        search_field: The field to search in the memory. It should be chosen from the attributes of the corresponding memory. For \"episodic\" memory, it can be 'summary', 'details'; for \"resource\" memory, it can be 'summary', 'content'; for \"procedural\" memory, it can be 'summary', 'steps'; for \"knowledge_vault\", it can be 'secret_value', 'caption'; for semantic memory, it can be 'name', 'summary', 'details'. For \"all\", it should also be \"null\" as the system will search all memories with default fields. \n        search_method: The method to search in the memory. Choose from:\n            - 'bm25': BM25 ranking-based full-text search (fast and effective for keyword-based searches)\n            - 'embedding': Vector similarity search using embeddings (most powerful, good for conceptual matches)\n            - 'string_match': Exact string match, can be used when you need to locate or compare text that must match exactly, character for character.\n\n    Returns:\n        str: Query result string\n    \"\"\"\n</code></pre></p>"},{"location":"architecture/multi-agent-system/#memory-managers","title":"Memory Managers","text":"<p>Each memory component has a dedicated agent that specializes in managing that specific type of information.</p>"},{"location":"architecture/multi-agent-system/#core-memory-manager","title":"Core Memory Manager","text":"<p>Manages: Personal preferences, user identity, essential facts</p> <p>Processing Logic:</p> <ul> <li>Identifies user preferences and personality traits</li> <li>Updates persona and human understanding blocks</li> <li>Maintains consistency across conversations</li> <li>Handles memory rewriting when blocks exceed 90% capacity</li> </ul>"},{"location":"architecture/multi-agent-system/#episodic-memory-manager","title":"Episodic Memory Manager","text":"<p>Manages: Time-based activities and events</p> <p>Processing Logic:</p> <ul> <li>Captures temporal context and user activities</li> <li>Creates event summaries with timestamps</li> <li>Tracks what the user has done and is currently doing</li> </ul>"},{"location":"architecture/multi-agent-system/#semantic-memory-manager","title":"Semantic Memory Manager","text":"<p>Manages: General knowledge and concepts about people and the world</p> <p>Processing Logic:</p> <ul> <li>Extracts factual information independent of time</li> <li>Stores concepts, definitions, and relationships</li> <li>Maintains knowledge about people, places, and things</li> <li>Links related concepts for better retrieval</li> </ul>"},{"location":"architecture/multi-agent-system/#procedural-memory-manager","title":"Procedural Memory Manager","text":"<p>Manages: Workflows and step-by-step processes</p> <p>Processing Logic:</p> <ul> <li>Identifies process patterns and workflows</li> <li>Stores step-by-step instructions</li> <li>Recognizes recurring task patterns</li> </ul>"},{"location":"architecture/multi-agent-system/#resource-memory-manager","title":"Resource Memory Manager","text":"<p>Manages: Documents, files, and content</p> <p>Processing Logic:</p> <ul> <li>Processes document content and context</li> <li>Stores full or partial content as needed</li> </ul>"},{"location":"architecture/multi-agent-system/#knowledge-vault-manager","title":"Knowledge Vault Manager","text":"<p>Manages: Structured data and credentials</p> <p>Processing Logic:</p> <ul> <li>Identifies sensitive information (passwords, API keys)</li> <li>Categorizes data by sensitivity level (<code>sensitivity</code> as <code>[low, medium, high]</code>)</li> <li>Maintains secure storage practices</li> <li>Prevents accidental exposure of <code>high</code> sensitivity data</li> </ul>"},{"location":"architecture/multi-agent-system/#workflow-coordination","title":"Workflow Coordination","text":""},{"location":"architecture/multi-agent-system/#1-input-processing-pipeline","title":"1. Input Processing Pipeline","text":"<pre><code>sequenceDiagram\n    participant U as User Input\n    participant M as Meta Agent\n    participant MM as Memory Managers\n    participant DB as Memory Base\n\n    U-&gt;&gt;M: Send content\n    M-&gt;&gt;M: Analyze content type\n    M-&gt;&gt;MM: Route to relevant managers\n    MM-&gt;&gt;MM: Process information\n    MM-&gt;&gt;DB: Update memory components\n    DB--&gt;&gt;MM: Confirm updates\n    MM--&gt;&gt;M: Processing complete\n    M--&gt;&gt;U: Acknowledgment</code></pre>"},{"location":"architecture/multi-agent-system/#2-memory-consolidation-process","title":"2. Memory Consolidation Process","text":"<p>Batch Processing:</p> <ul> <li>Agents accumulate information until reaching threshold</li> <li>Trigger parallel processing for efficiency</li> <li>Single function call per agent for comprehensive updates</li> </ul> <p>Smart Routing:</p> <ul> <li>Meta Agent analyze the content to determine distribution, preventing unnecessary processing and maintaining efficiency</li> <li>(Double Check) The specific memory manager can skip updates if no relevant information detected</li> </ul>"},{"location":"architecture/multi-agent-system/#3-conversational-retrieval-system","title":"3. Conversational Retrieval System","text":"<pre><code>sequenceDiagram\n    participant U as User Query\n    participant C as Chat Agent\n    participant S as search_in_memory()\n    participant DB as Memory Base\n    participant R as Response\n\n    U-&gt;&gt;C: Ask question\n    C-&gt;&gt;S: Call search function\n    S-&gt;&gt;DB: Query all memory types\n    DB--&gt;&gt;S: Return relevant results\n    S--&gt;&gt;C: Consolidated results\n    C-&gt;&gt;C: Synthesize response\n    C-&gt;&gt;R: Generate answer\n    R--&gt;&gt;U: Intelligent response</code></pre>"},{"location":"architecture/multi-agent-system/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"architecture/multi-agent-system/#concurrent-processing","title":"Concurrent Processing","text":"<ul> <li>Memory Managers work independently but share the same memory base</li> <li>Parallel processing of different memory types</li> </ul>"},{"location":"architecture/multi-agent-system/#single-function-call-architecture","title":"Single Function Call Architecture","text":"<ul> <li>Each agent makes comprehensive updates in a single function call</li> <li>Reduces database round trips and improves performance</li> <li>Maintains consistency across memory components</li> </ul>"},{"location":"architecture/multi-agent-system/#error-handling-and-resilience","title":"Error Handling and Resilience","text":""},{"location":"architecture/multi-agent-system/#graceful-degradation","title":"Graceful Degradation","text":"<ul> <li>Agents can skip updates if processing fails</li> <li>System continues operating even if individual agents encounter errors</li> <li>Automatic retry mechanisms for transient failures</li> </ul>"},{"location":"architecture/multi-agent-system/#data-consistency","title":"Data Consistency","text":"<ul> <li>Shared memory base ensures consistency across agents</li> <li>Transaction-based updates prevent data corruption</li> <li>Automatic rollback on processing failures</li> </ul>"},{"location":"architecture/multi-agent-system/#whats-next","title":"What's Next?","text":"<p>Dive deeper into the memory components that power this system:</p> <p>Memory Components \u2192 </p>"},{"location":"architecture/search-capabilities/","title":"Search Capabilities","text":"<p>MIRIX provides multiple sophisticated search methods for retrieving information from its 5 memory components (Procedural, Knowledge Vault, Episodic, Semantic, Resource), with PostgreSQL-native full-text search as the primary implementation for optimal performance and scalability.</p>"},{"location":"architecture/search-capabilities/#search-methods-overview","title":"Search Methods Overview","text":"<p>MIRIX supports three distinct search methods, each optimized for different use cases:</p> Method Description Best For Performance <code>bm25</code> RECOMMENDED - PostgreSQL native full-text search Most queries, production use Excellent <code>embedding</code> Vector similarity search using embeddings Semantic similarity, conceptual queries Good <code>string_match</code> Simple string containment search Exact text matching Fast"},{"location":"architecture/search-capabilities/#postgresql-native-bm25-implementation","title":"PostgreSQL Native BM25 Implementation","text":""},{"location":"architecture/search-capabilities/#architecture","title":"Architecture","text":"<p>All 5 memory managers (Procedural, Knowledge Vault, Episodic, Semantic, Resource) use PostgreSQL's native <code>ts_rank_cd</code> function for BM25-like scoring directly in the database.</p> <pre><code>graph TB\n    A[Search Query] --&gt; B[Text Cleaning &amp; Tokenization]\n    B --&gt; C[TSQuery Generation with Prefix Matching]\n    C --&gt; D[AND Query Attempt]\n    D --&gt; E{Sufficient Results?}\n    E --&gt;|Yes| F[Return AND Results]\n    E --&gt;|No| G[OR Query Fallback]\n    G --&gt; H[GIN Index Lookup]\n    H --&gt; I[ts_rank_cd Scoring]\n    I --&gt; J[Memory-Type-Specific Field Processing]\n    J --&gt; K[Ranked Results]</code></pre>"},{"location":"architecture/search-capabilities/#key-technical-features","title":"Key Technical Features","text":""},{"location":"architecture/search-capabilities/#smart-query-preprocessing","title":"Smart Query Preprocessing","text":"<ul> <li>Text Cleaning: Removes punctuation, normalizes whitespace, converts to lowercase</li> <li>Tokenization: Splits into meaningful tokens, filters out very short words</li> <li>TSQuery Creation: Generates both exact and prefix matching queries for better recall</li> <li>Special Character Escaping: Safely handles PostgreSQL tsquery special characters</li> </ul> <pre><code># Example preprocessing flow\n\"machine-learning AI!\" \u2192 \"machine learning ai\" \u2192 [\"machine\", \"learning\", \"ai\"]\n\u2192 \"(machine | machine:*) &amp; (learning | learning:*) &amp; (ai | ai:*)\"\n</code></pre>"},{"location":"architecture/search-capabilities/#advanced-query-logic-with-fallback","title":"Advanced Query Logic with Fallback","text":"<pre><code>-- Step 1: Try precise AND query for high precision\nSELECT *, ts_rank_cd(tsvector_field, to_tsquery('english', :and_query), 32) as rank\nFROM memory_table \nWHERE tsvector_field @@ to_tsquery('english', :and_query)\nORDER BY rank DESC, created_at DESC\nLIMIT 50;\n\n-- Step 2: If insufficient results, fallback to OR query for better recall\nSELECT *, ts_rank_cd(tsvector_field, to_tsquery('english', :or_query), 32) as rank  \nFROM memory_table\nWHERE tsvector_field @@ to_tsquery('english', :or_query)\nORDER BY rank DESC, created_at DESC\nLIMIT 50;\n\n-- Step 3: Ultimate fallback to simple ILIKE if tsquery fails\nSELECT * FROM memory_table\nWHERE lower(field) LIKE lower('%query%')\nORDER BY created_at DESC\nLIMIT 50;\n</code></pre>"},{"location":"architecture/search-capabilities/#memory-type-specific-field-implementation","title":"Memory-Type-Specific Field Implementation","text":"<p>Each memory type has different searchable fields and special handling:</p> <p>Procedural Memory Fields:</p> <ul> <li><code>summary</code>: Process description - <code>to_tsvector('english', coalesce(summary, ''))</code></li> <li><code>steps</code>: JSON array with special processing - <code>to_tsvector('english', coalesce(regexp_replace(steps::text, '[\\\"\\\\[\\\\],]', ' ', 'g'), ''))</code></li> <li><code>entry_type</code>: Type of procedure - <code>to_tsvector('english', coalesce(entry_type, ''))</code></li> </ul> <p>Knowledge Vault Fields:</p> <ul> <li><code>caption</code>: Description of stored item - <code>to_tsvector('english', coalesce(caption, ''))</code></li> <li><code>secret_value</code>: Actual stored value - <code>to_tsvector('english', coalesce(secret_value, ''))</code></li> <li>Additional: <code>entry_type</code>, <code>source</code>, <code>sensitivity</code> (with filtering support)</li> </ul> <p>Episodic Memory Fields:</p> <ul> <li><code>summary</code>: Brief event description - <code>to_tsvector('english', coalesce(summary, ''))</code></li> <li><code>details</code>: Comprehensive event information - <code>to_tsvector('english', coalesce(details, ''))</code></li> <li><code>actor</code>: Who performed the action - <code>to_tsvector('english', coalesce(actor, ''))</code></li> <li><code>event_type</code>: Category of event - <code>to_tsvector('english', coalesce(event_type, ''))</code></li> </ul> <p>Semantic Memory Fields:</p> <ul> <li><code>name</code>: Concept or object name - <code>to_tsvector('english', coalesce(name, ''))</code></li> <li><code>summary</code>: Concise explanation - <code>to_tsvector('english', coalesce(summary, ''))</code></li> <li><code>details</code>: Extended description - <code>to_tsvector('english', coalesce(details, ''))</code></li> <li><code>source</code>: Knowledge origin - <code>to_tsvector('english', coalesce(source, ''))</code></li> </ul> <p>Resource Memory Fields:</p> <ul> <li><code>title</code>: Resource name - <code>to_tsvector('english', coalesce(title, ''))</code></li> <li><code>summary</code>: Brief description - <code>to_tsvector('english', coalesce(summary, ''))</code></li> <li><code>content</code>: Full document content - <code>to_tsvector('english', coalesce(content, ''))</code></li> <li><code>resource_type</code>: File format type - <code>to_tsvector('english', coalesce(resource_type, ''))</code></li> </ul> <p>Special JSON Array Handling (Procedural Memory Only): <pre><code>-- Transforms: [\"step 1\", \"step 2\", \"step 3\"]\n-- Into: \"step 1 step 2 step 3\" for full-text search\nregexp_replace(steps::text, '[\\\"\\\\[\\\\],]', ' ', 'g')\n</code></pre></p>"},{"location":"architecture/search-capabilities/#field-weighting-system-multi-field-search-only","title":"Field Weighting System (Multi-Field Search Only)","text":"<p>When no specific <code>search_field</code> is provided, each memory type uses different field weighting:</p> <p>Procedural Memory: <pre><code>setweight(to_tsvector('english', coalesce(summary, '')), 'A') ||\nsetweight(to_tsvector('english', coalesce(regexp_replace(steps::text, '[\\\"\\\\[\\\\],]', ' ', 'g'), '')), 'B') ||\nsetweight(to_tsvector('english', coalesce(entry_type, '')), 'C')\n</code></pre></p> <p>Knowledge Vault (2-tier weighting): <pre><code>setweight(to_tsvector('english', coalesce(caption, '')), 'A') ||\nsetweight(to_tsvector('english', coalesce(secret_value, '')), 'B')\n</code></pre></p> <p>Episodic Memory: <pre><code>setweight(to_tsvector('english', coalesce(summary, '')), 'A') ||\nsetweight(to_tsvector('english', coalesce(details, '')), 'B') ||\nsetweight(to_tsvector('english', coalesce(actor, '')), 'C') ||\nsetweight(to_tsvector('english', coalesce(event_type, '')), 'D')\n</code></pre></p> <p>Semantic Memory: <pre><code>setweight(to_tsvector('english', coalesce(name, '')), 'A') ||\nsetweight(to_tsvector('english', coalesce(summary, '')), 'B') ||\nsetweight(to_tsvector('english', coalesce(details, '')), 'C') ||\nsetweight(to_tsvector('english', coalesce(source, '')), 'D')\n</code></pre></p> <p>Resource Memory: <pre><code>setweight(to_tsvector('english', coalesce(title, '')), 'A') ||\nsetweight(to_tsvector('english', coalesce(summary, '')), 'B') ||\nsetweight(to_tsvector('english', coalesce(content, '')), 'C') ||\nsetweight(to_tsvector('english', coalesce(resource_type, '')), 'D')\n</code></pre></p> <p>Note: Field weighting is only applied during multi-field searches. Single-field searches directly query the specified field without weighting.</p>"},{"location":"architecture/search-capabilities/#special-features","title":"Special Features","text":"<p>Knowledge Vault Sensitivity Filtering: Knowledge Vault has an additional <code>sensitivity</code> parameter for security-based filtering: <pre><code># Filter by sensitivity levels\nresults = knowledge_vault_manager.list_knowledge(\n    query=\"password\",\n    sensitivity=[\"low\", \"medium\"]  # Exclude \"high\" sensitivity items\n)\n</code></pre></p>"},{"location":"architecture/search-capabilities/#document-length-normalization","title":"Document Length Normalization","text":"<p>Uses <code>ts_rank_cd</code> with normalization parameter 32 for optimal BM25-like scoring: <pre><code>ts_rank_cd(search_vector, query, 32)\n-- 32 = normalize by document length + use logarithmic normalization + unique word count\n</code></pre></p>"},{"location":"architecture/search-capabilities/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"architecture/search-capabilities/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Zero In-Memory Loading: Eliminates the need to load all documents into Python memory</li> <li>Database-Level Processing: All ranking and filtering done at the PostgreSQL level</li> <li>Scalable Architecture: Performance scales with your PostgreSQL setup</li> </ul>"},{"location":"architecture/search-capabilities/#search-field-specifications","title":"Search Field Specifications","text":"<p>Each of the 5 memory types supports field-specific searches:</p>"},{"location":"architecture/search-capabilities/#procedural-memory-workflows-procedures","title":"Procedural Memory (Workflows &amp; Procedures)","text":"<ul> <li><code>summary</code>: Process description</li> <li><code>steps</code>: Detailed instructions (JSON array with special processing)</li> <li><code>entry_type</code>: Type of procedure (workflow/guide/script)</li> </ul>"},{"location":"architecture/search-capabilities/#knowledge-vault-sensitive-information","title":"Knowledge Vault (Sensitive Information)","text":"<ul> <li><code>caption</code>: Description of stored item</li> <li><code>secret_value</code>: Actual stored value (credentials, tokens, etc.)</li> <li><code>entry_type</code>: Type of data (credential/bookmark/contact)</li> <li><code>source</code>: Data origin</li> <li><code>sensitivity</code>: Security classification (low/medium/high) - supports filtering</li> </ul>"},{"location":"architecture/search-capabilities/#episodic-memory-events-activities","title":"Episodic Memory (Events &amp; Activities)","text":"<ul> <li><code>summary</code>: Brief event description</li> <li><code>details</code>: Comprehensive event information  </li> <li><code>actor</code>: Who performed the action (user/assistant)</li> <li><code>event_type</code>: Category of event</li> </ul>"},{"location":"architecture/search-capabilities/#semantic-memory-concepts-knowledge","title":"Semantic Memory (Concepts &amp; Knowledge)","text":"<ul> <li><code>name</code>: Concept or object name</li> <li><code>summary</code>: Concise explanation</li> <li><code>details</code>: Extended description</li> <li><code>source</code>: Knowledge origin</li> </ul>"},{"location":"architecture/search-capabilities/#resource-memory-documents-files","title":"Resource Memory (Documents &amp; Files)","text":"<ul> <li><code>title</code>: Resource name</li> <li><code>summary</code>: Brief description with context</li> <li><code>content</code>: Full document content</li> <li><code>resource_type</code>: File format type (doc/markdown/pdf/etc.)</li> </ul>"},{"location":"architecture/search-capabilities/#usage-examples","title":"Usage Examples","text":""},{"location":"architecture/search-capabilities/#basic-search","title":"Basic Search","text":"<pre><code>from mirix.agent import AgentWrapper\n\nagent = AgentWrapper(\"./configs/mirix.yaml\")\n\n# Basic search across all memory types\nresults = agent.search_memory(\"machine learning algorithms\")\n</code></pre>"},{"location":"architecture/search-capabilities/#memory-type-specific-search-examples","title":"Memory-Type-Specific Search Examples","text":"<pre><code># Procedural Memory - Find workflows\nresults = procedural_memory_manager.list_procedures(\n    agent_state=agent_state,\n    query=\"deployment process\",\n    search_method=\"bm25\",\n    search_field=\"summary\",\n    limit=20\n)\n\n# Knowledge Vault - Find credentials with sensitivity filtering\nresults = knowledge_vault_manager.list_knowledge(\n    agent_state=agent_state,\n    query=\"database password\",\n    search_method=\"bm25\",\n    sensitivity=[\"low\", \"medium\"],  # Exclude high sensitivity\n    limit=10\n)\n\n# Episodic Memory - Find recent activities\nresults = episodic_memory_manager.list_episodic_memory(\n    agent_state=agent_state,\n    query=\"code review\",\n    search_method=\"bm25\",\n    search_field=\"details\",\n    limit=15\n)\n\n# Semantic Memory - Find concepts\nresults = semantic_memory_manager.list_semantic_items(\n    agent_state=agent_state,\n    query=\"machine learning\",\n    search_method=\"embedding\",\n    search_field=\"name\",\n    limit=25\n)\n\n# Resource Memory - Find documents\nresults = resource_memory_manager.list_resources(\n    agent_state=agent_state,\n    query=\"API documentation\",\n    search_method=\"bm25\",\n    search_field=\"content\",\n    limit=30\n)\n</code></pre>"},{"location":"architecture/search-capabilities/#advanced-features","title":"Advanced Features","text":""},{"location":"architecture/search-capabilities/#special-memory-type-features","title":"Special Memory-Type Features","text":"<p>Knowledge Vault Sensitivity Filtering: <pre><code># Filter by sensitivity levels for security\nresults = knowledge_vault_manager.list_knowledge(\n    query=\"database credentials\",\n    sensitivity=[\"low\", \"medium\"]  # Exclude high sensitivity items\n)\n</code></pre></p> <p>Episodic Memory Time-Based Search: <pre><code># Search within specific time ranges\nresults = episodic_memory_manager.list_episodic_memory_around_timestamp(\n    agent_state=agent_state,\n    start_time=yesterday,\n    end_time=today\n)\n</code></pre></p> <p>Procedural Memory JSON Array Processing: <pre><code># Search within step-by-step instructions\nresults = procedural_memory_manager.list_procedures(\n    query=\"git deployment\",\n    search_field=\"steps\",  # Searches within JSON array of steps\n    search_method=\"bm25\"\n)\n</code></pre></p>"},{"location":"architecture/search-capabilities/#whats-next","title":"What's Next?","text":"<p>Learn how to use these search capabilities in practice:</p> <p>User Guide \u2192 </p>"},{"location":"getting-started/overview/","title":"Overview","text":"<p>Mirix is a memory system you can plug into any agent stack. It captures and structures information over time, then makes it retrievable through a unified API.</p>"},{"location":"getting-started/overview/#what-mirix-provides","title":"What Mirix Provides","text":"<ul> <li>Multi-agent memory pipeline that writes to six memory components (core, episodic, semantic, procedural, resource, knowledge)</li> <li>Pluggable client API that works with hosted or self-hosted backends</li> <li>Search and retrieval across memory types with BM25 and embedding-based queries</li> <li>Local-first data storage and configurable retention policies</li> </ul>"},{"location":"getting-started/overview/#product-direction","title":"Product Direction","text":"<p>Starting with version 0.1.6 on the <code>main</code> branch, Mirix is no longer a desktop personal assistant. The desktop agent has been deprecated and lives on the <code>desktop-agent</code> branch.</p>"},{"location":"getting-started/overview/#where-to-go-next","title":"Where to Go Next","text":"<ul> <li>Quickstart: Get to a working client in minutes.</li> <li>Memory Write: Configure LLMs, embeddings, and retention.</li> <li>Memory Search: Query memories with the search API.</li> </ul> <p> Back to Home Next: Quickstart </p>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>This quickstart uses the hosted API. If you are self-hosting, set <code>base_url</code> when creating the client.</p>"},{"location":"getting-started/quickstart/#install","title":"Install","text":"<pre><code>pip install mirix==0.1.6\n</code></pre>"},{"location":"getting-started/quickstart/#initialize-the-client","title":"Initialize the Client","text":"<pre><code>from mirix import MirixClient\n\nclient = MirixClient(api_key=\"your_api_key_here\")\n\nclient.initialize_meta_agent(\n    provider=\"openai\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#add-memories","title":"Add Memories","text":"<pre><code>client.add(\n    user_id=\"demo-user\",\n    messages=[\n        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"The moon now has a president.\"}]},\n        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"Noted.\"}]},\n    ],\n)\n</code></pre>"},{"location":"getting-started/quickstart/#retrieve-with-conversation-context","title":"Retrieve with Conversation Context","text":"<pre><code>memories = client.retrieve_with_conversation(\n    user_id=\"demo-user\",\n    messages=[\n        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What did we discuss recently?\"}]},\n    ],\n    limit=5,\n)\nprint(memories)\n</code></pre>"},{"location":"getting-started/quickstart/#self-hosted-setup","title":"Self-Hosted Setup","text":""},{"location":"getting-started/quickstart/#option-a-local-installation","title":"Option A: Local Installation","text":""},{"location":"getting-started/quickstart/#step-1-install-dependencies","title":"Step 1: Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-set-up-database","title":"Step 2: Set Up Database","text":"<p>You have two database options:</p> SQLite (Default - No Setup Required)PostgreSQL (Recommended for Production) <p>If you don't set up PostgreSQL, Mirix will automatically use SQLite. No additional configuration needed!</p> <p>Simply proceed to Step 3 and skip the PostgreSQL environment variables.</p> <p>SQLite Limitations</p> <p>SQLite works but has limitations in concurrent access and advanced search capabilities compared to PostgreSQL.</p> <p>PostgreSQL provides better performance, scalability, and vector search capabilities.</p> <p>Install PostgreSQL and pgvector:</p> macOS (Homebrew)Ubuntu/DebianWindows <pre><code># Install PostgreSQL and pgvector\nbrew install postgresql@17 pgvector\n\n# Start PostgreSQL service\nbrew services start postgresql@17\n\n# Add PostgreSQL to your PATH\nexport PATH=\"$(brew --prefix postgresql@17)/bin:$PATH\"\n</code></pre> <pre><code># Install PostgreSQL\nsudo apt-get update\nsudo apt-get install postgresql postgresql-contrib\n\n# Install pgvector (see https://github.com/pgvector/pgvector for latest instructions)\n\n# Start PostgreSQL\nsudo systemctl start postgresql\n</code></pre> <ol> <li>Download PostgreSQL from postgresql.org</li> <li>Install pgvector following the official guide</li> <li>Start PostgreSQL service</li> </ol> <p>Create Database and Enable Extensions:</p> <pre><code># Create the mirix database\ncreatedb mirix\n\n# Enable pgvector extension\npsql -U $(whoami) -d mirix -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n</code></pre> <p>Configure PostgreSQL in Step 3 below by adding the database URI to your environment variables.</p>"},{"location":"getting-started/quickstart/#step-3-configure-environment","title":"Step 3: Configure Environment","text":"<p>Create a <code>.env</code> file or set environment variables:</p> Using SQLite (Default)Using PostgreSQLOptional: Add Redis <pre><code># Required: At least one LLM provider API key\nexport OPENAI_API_KEY=sk-your-openai-api-key\n</code></pre> <p>That's it! SQLite will be used automatically. Proceed to Step 4.</p> <pre><code># Required: At least one LLM provider API key\nexport OPENAI_API_KEY=sk-your-openai-api-key\n\n# Required: PostgreSQL connection\n# Replace 'your_username' with your system username (find it with: echo $(whoami))\nexport MIRIX_PG_URI=postgresql+pg8000://your_username@localhost:5432/mirix\n</code></pre> <p>Add Redis configuration to either setup above:</p> <pre><code># Optional: Redis for caching (improves performance)\nexport MIRIX_REDIS_ENABLED=true\nexport MIRIX_REDIS_HOST=localhost\nexport MIRIX_REDIS_PORT=6379\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-start-the-backend","title":"Step 4: Start the Backend","text":"<p>In terminal 1: <pre><code>python scripts/start_server.py\n# or for development with auto-reload:\npython scripts/start_server.py --reload\n</code></pre></p> <p>The API server will start at <code>http://localhost:8531</code></p>"},{"location":"getting-started/quickstart/#step-5-start-the-dashboard-optional","title":"Step 5: Start the Dashboard (Optional)","text":"<p>In terminal 2: <pre><code>cd dashboard\nnpm install\nnpm run dev\n</code></pre></p> <p>The dashboard will be available at <code>http://localhost:5173</code></p>"},{"location":"getting-started/quickstart/#step-6-create-an-api-key","title":"Step 6: Create an API Key","text":"<ol> <li>Open the dashboard at <code>http://localhost:5173</code></li> <li>Create an API key for authentication</li> <li>Set it as an environment variable: <pre><code>export MIRIX_API_KEY=your-generated-api-key\n</code></pre></li> </ol>"},{"location":"getting-started/quickstart/#step-7-use-the-client","title":"Step 7: Use the Client","text":"<pre><code>from mirix import MirixClient\n\nclient = MirixClient(\n    api_key=\"your-api-key\",  # or use MIRIX_API_KEY env var\n    base_url=\"http://localhost:8531\",\n)\n\nclient.initialize_meta_agent(\n    config={\n        \"llm_config\": {\n            \"model\": \"gpt-4o-mini\",\n            \"model_endpoint_type\": \"openai\",\n            \"model_endpoint\": \"https://api.openai.com/v1\",\n            \"context_window\": 128000,\n        },\n        \"build_embeddings_for_memory\": True,\n        \"embedding_config\": {\n            \"embedding_model\": \"text-embedding-3-small\",\n            \"embedding_endpoint\": \"https://api.openai.com/v1\",\n            \"embedding_endpoint_type\": \"openai\",\n            \"embedding_dim\": 1536,\n        },\n        \"meta_agent_config\": {\n            \"agents\": [\n                \"core_memory_agent\",\n                \"resource_memory_agent\",\n                \"semantic_memory_agent\",\n                \"episodic_memory_agent\",\n                \"procedural_memory_agent\",\n                \"knowledge_memory_agent\",\n                \"reflexion_agent\",\n                \"background_agent\",\n            ],\n            \"memory\": {\n                \"core\": [\n                    {\"label\": \"human\", \"value\": \"\"},\n                    {\"label\": \"persona\", \"value\": \"I am a helpful assistant.\"},\n                ],\n                \"decay\": {\n                    \"fade_after_days\": 30,\n                    \"expire_after_days\": 90,\n                },\n            },\n        },\n    }\n)\n</code></pre>"},{"location":"getting-started/quickstart/#option-b-docker-setup-postgresql-included","title":"Option B: Docker Setup (PostgreSQL Included)","text":"<p>Docker automatically sets up PostgreSQL, Redis, and all services for you - no manual database configuration needed!</p>"},{"location":"getting-started/quickstart/#step-1-copy-environment-file","title":"Step 1: Copy Environment File","text":"<pre><code>cp docker/env.example .env\n# Edit .env and set at least OPENAI_API_KEY\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-start-all-services","title":"Step 2: Start All Services","text":"<pre><code>docker-compose up -d\n</code></pre> <p>This will automatically start and configure: - PostgreSQL with pgvector (port 5432) - automatically configured - Redis Stack (port 6379) - for caching - Mirix API (port 8531) - backend server - Dashboard (port 5173) - web UI</p> <p>Database Included</p> <p>PostgreSQL is automatically set up and configured in Docker - no manual database setup required!</p>"},{"location":"getting-started/quickstart/#step-3-access-the-dashboard","title":"Step 3: Access the Dashboard","text":"<p>Open <code>http://localhost:5173</code> and create an API key.</p>"},{"location":"getting-started/quickstart/#step-4-use-the-client","title":"Step 4: Use the Client","text":"<p>Same as Option A above, using <code>base_url=\"http://localhost:8531\"</code>.</p>"},{"location":"getting-started/quickstart/#helpful-resources","title":"Helpful Resources","text":"<ul> <li>API Documentation: <code>http://localhost:8531/docs</code> (Swagger UI)</li> <li>Alternative API Docs: <code>http://localhost:8531/redoc</code></li> <li>Full Examples: See <code>samples/run_client.py</code> in the Mirix repo</li> <li>Docker Documentation: See <code>docker/README.md</code> in the Mirix repo</li> </ul> <p> Overview Next: Memory Write </p>"},{"location":"legacy/","title":"Legacy Documentation","text":"<p>This section contains the original documentation for the deprecated desktop assistant and its supporting backend. It is preserved for reference if you are working from the <code>desktop-agent</code> branch.</p>"},{"location":"legacy/#what-moved-here","title":"What Moved Here","text":"<ul> <li>Desktop app setup and UI walkthroughs</li> <li>Backend usage notes for the old assistant stack</li> <li>Advanced topics like backups, performance, and security</li> </ul>"},{"location":"legacy/#recommended-path","title":"Recommended Path","text":"<p>If you are using the current memory system release (0.1.6+ on <code>main</code>), start with <code>getting-started/overview.md</code> instead.</p>"},{"location":"legacy/advanced/backup-restore/","title":"Backup &amp; Restore","text":"<p>MIRIX provides simple backup and restore functionality for your agent data, ensuring your memories and configurations are safe and portable.</p>"},{"location":"legacy/advanced/backup-restore/#overview","title":"Overview","text":"<p>The backup system automatically handles both PostgreSQL and SQLite databases, preserving all your conversations, memories, and agent configurations in a portable format.</p>"},{"location":"legacy/advanced/backup-restore/#prerequisites","title":"Prerequisites","text":""},{"location":"legacy/advanced/backup-restore/#for-postgresql-backups","title":"For PostgreSQL Backups","text":"<p>Since backups use <code>pg_dump</code> to save the database, ensure PostgreSQL tools are accessible:</p> <pre><code># macOS with Homebrew\nexport PATH=\"$(brew --prefix postgresql@17)/bin:$PATH\"\n\n# Verify pg_dump is available\nwhich pg_dump\n</code></pre> <p>PATH Configuration Required</p> <p>Without proper PATH configuration, PostgreSQL backup operations will fail with \"pg_dump: No such file or directory\" error.</p>"},{"location":"legacy/advanced/backup-restore/#for-sqlite-backups","title":"For SQLite Backups","text":"<p>No additional setup required - SQLite backups work out of the box.</p>"},{"location":"legacy/advanced/backup-restore/#creating-backups","title":"Creating Backups","text":""},{"location":"legacy/advanced/backup-restore/#basic-backup","title":"Basic Backup","text":"<pre><code>from mirix.agent import AgentWrapper\n\n# Initialize agent\nagent = AgentWrapper(\"./configs/mirix.yaml\")\n\n# Create backup\nresult = agent.save_agent(\"./my_backup\")\nprint(result['message'])  # \"Agent state saved successfully...\"\n</code></pre> <p>That's it! The backup is created in the specified directory.</p>"},{"location":"legacy/advanced/backup-restore/#backup-structure","title":"Backup Structure","text":"<p>The backup structure depends on your database type:</p>"},{"location":"legacy/advanced/backup-restore/#postgresql-backup-structure","title":"PostgreSQL Backup Structure","text":"<pre><code>my_backup/\n\u251c\u2500\u2500 agent_config.json      # Agent configuration\n\u251c\u2500\u2500 mirix_database.dump    # Binary database dump\n\u2514\u2500\u2500 mirix_database.sql     # SQL database dump\n</code></pre>"},{"location":"legacy/advanced/backup-restore/#sqlite-backup-structure","title":"SQLite Backup Structure","text":"<pre><code>my_backup/\n\u251c\u2500\u2500 agent_config.json      # Agent configuration\n\u2514\u2500\u2500 sqlite.db             # Complete SQLite database file\n</code></pre>"},{"location":"legacy/advanced/backup-restore/#restoring-from-backups","title":"Restoring from Backups","text":""},{"location":"legacy/advanced/backup-restore/#method-1-restore-during-initialization-recommended","title":"Method 1: Restore During Initialization (Recommended)","text":"<pre><code># Initialize agent with backup data\nagent = AgentWrapper(\"./configs/mirix.yaml\", \"./my_backup\")\n\nprint(\"Agent restored from backup\")\n</code></pre>"},{"location":"legacy/advanced/backup-restore/#method-2-load-backup-after-creation","title":"Method 2: Load Backup After Creation","text":"<pre><code># Create agent normally\nagent = AgentWrapper(\"./configs/mirix.yaml\")\n\n# Then load backup data\nconfig_path = \"./my_backup/mirix_config.yaml\"\nself._agent = AgentWrapper(str(config_path), load_from=\"./my_backup\")\n\nif result['success']:\n    print(\"Backup restored successfully\")\nelse:\n    print(f\"Restore failed: {result['error']}\")\n</code></pre>"},{"location":"legacy/advanced/backup-restore/#troubleshooting","title":"Troubleshooting","text":"\\\"pg_dump: No such file or directory\\\" error <p>PostgreSQL tools are not in your PATH: <pre><code># Add PostgreSQL to PATH\nexport PATH=\"$(brew --prefix postgresql@17)/bin:$PATH\"\n\n# Add to your shell profile for persistence\necho 'export PATH=\"$(brew --prefix postgresql@17)/bin:$PATH\"' &gt;&gt; ~/.zshrc\n</code></pre></p> \\\"Permission denied\\\" during backup <p>Check database permissions: <pre><code>-- Connect to your database\npsql -U $(whoami) -d mirix\n\n-- Check permissions\n\\du\n</code></pre></p> Backup directory already exists <p>Choose a different backup path or remove the existing directory: <pre><code>import shutil\nimport os\n\nbackup_path = \"./my_backup\"\nif os.path.exists(backup_path):\n    shutil.rmtree(backup_path)\n\n# Now create backup\nresult = agent.save_agent(backup_path)\n</code></pre></p> Restore fails <p>Verify the backup directory contains the required files: <pre><code>import os\n\nbackup_path = \"./my_backup\"\n\n# Check for required files\nconfig_exists = os.path.exists(f\"{backup_path}/agent_config.json\")\n\n# For PostgreSQL\npg_dump_exists = os.path.exists(f\"{backup_path}/mirix_database.dump\")\npg_sql_exists = os.path.exists(f\"{backup_path}/mirix_database.sql\")\n\n# For SQLite\nsqlite_exists = os.path.exists(f\"{backup_path}/sqlite.db\")\n\nprint(f\"Config file: {config_exists}\")\nprint(f\"PostgreSQL dumps: {pg_dump_exists and pg_sql_exists}\")\nprint(f\"SQLite database: {sqlite_exists}\")\n</code></pre></p>"},{"location":"legacy/advanced/backup-restore/#migration-between-systems","title":"Migration Between Systems","text":""},{"location":"legacy/advanced/backup-restore/#export-for-migration","title":"Export for Migration","text":"<p>To move your MIRIX agent to a new system:</p> <ol> <li> <p>Create a backup on the source system:    <pre><code>agent = AgentWrapper(\"./configs/mirix.yaml\")\nresult = agent.save_agent(\"./migration_backup\")\n</code></pre></p> </li> <li> <p>Copy the backup to the target system:    <pre><code># Copy backup directory to new system\nscp -r ./migration_backup user@new-system:/path/to/mirix/\n</code></pre></p> </li> <li> <p>Restore on the target system:    <pre><code># On the new system\nagent = AgentWrapper(\"./configs/mirix.yaml\", \"./migration_backup\")\n</code></pre></p> </li> </ol>"},{"location":"legacy/advanced/backup-restore/#whats-next","title":"What's Next?","text":"<p>Learn about contributing:</p> <p>Contributing \u2192</p>"},{"location":"legacy/advanced/performance/","title":"Performance","text":"<p>MIRIX delivers exceptional performance through intelligent memory consolidation, optimized search algorithms, and efficient data processing.</p>"},{"location":"legacy/advanced/performance/#experimental-setup","title":"Experimental Setup","text":"<p>We evaluate MIRIX on two comprehensive datasets to demonstrate its superior performance across different scenarios:</p>"},{"location":"legacy/advanced/performance/#locomo-dataset","title":"LOCOMO Dataset","text":"<p>Following Mem0, we use the LOCOMO dataset for vertical comparison between MIRIX and existing memory systems. LOCOMO contains:</p> <ul> <li>10 conversations with 600 dialogues each</li> <li>26,000 tokens on average per conversation  </li> <li>200 questions per conversation across multiple categories:</li> <li>Single-hop questions</li> <li>Multi-hop questions</li> <li>Temporal questions</li> <li>Open-domain questions</li> </ul>"},{"location":"legacy/advanced/performance/#screenshotvqa-dataset","title":"ScreenshotVQA Dataset","text":"<p>We collected a novel dataset containing three PhD students' computer activities:</p> <ul> <li>Student 1: 5,886 screenshots (1 day, heavy usage)</li> <li>Student 2: 18,178 screenshots (3 weeks, moderate usage)  </li> <li>Student 3: 5,349 screenshots (6 weeks, light usage)</li> <li>Total questions: 87 manually created and verified questions</li> </ul> <p>Screenshots were captured every second, with duplicate filtering (similarity &gt; 0.99).</p>"},{"location":"legacy/advanced/performance/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>LLM-as-a-Judge: Using GPT-4.1 to evaluate response quality</li> <li>Accuracy: Percentage of correctly answered questions</li> <li>Storage: Memory footprint comparison</li> </ul>"},{"location":"legacy/advanced/performance/#experimental-results","title":"Experimental Results","text":""},{"location":"legacy/advanced/performance/#locomo-dataset-results","title":"LOCOMO Dataset Results","text":"Method Single Hop Multi-Hop Open Domain Temporal Overall GPT-4o-mini backbone A-Mem 39.79 18.85 54.05 49.91 48.38 LangMem 62.23 47.92 71.12 23.43 58.10 OpenAI 63.79 42.92 62.29 21.71 52.90 Mem0 67.13 51.15 72.93 55.51 66.88 Mem0\u1d4d 65.71 47.19 75.71 58.13 68.44 Memobase 63.83 52.08 71.82 80.37 70.91 Zep 74.11 66.04 67.71 79.76 75.14 GPT-4.1-mini backbone LangMem 74.47 61.06 67.71 86.92 78.05 RAG-500 37.94 37.69 48.96 61.83 51.62 Zep 79.43 69.16 73.96 83.33 79.09 Mem0 62.41 57.32 44.79 66.47 62.47 MIRIX 85.11 83.70 65.62 88.39 85.38 Full-Context 88.53 77.70 71.88 92.70 87.52 <p>LLM-as-a-Judge scores (%, higher is better) for each question type in the LOCOMO dataset. Full-Context represents the upper-bound performance.</p>"},{"location":"legacy/advanced/performance/#screenshotvqa-results","title":"ScreenshotVQA Results","text":"Method Student 1 Student 2 Student 3 Overall Acc \u2191 Storage \u2193 Acc \u2191 Storage \u2193 Acc \u2191 Storage \u2193 Acc \u2191 Storage \u2193 Gemini 0.00% 142.10MB 9.52% 438.86MB 25.45% 129.14MB 11.66% 236.70MB SigLIP@50 36.36% 22.55GB 41.38% 19.88GB 54.55% 2.82GB 44.10% 15.07GB MIRIX 54.55% 20.57MB 56.67% 19.83MB 67.27% 7.28MB 59.50% 15.89MB"},{"location":"legacy/advanced/performance/#key-performance-insights","title":"Key Performance Insights","text":""},{"location":"legacy/advanced/performance/#superior-multi-hop-reasoning","title":"Superior Multi-Hop Reasoning","text":"<p>MIRIX shows the largest improvement in multi-hop questions, outperforming baselines by more than 22 points. This highlights the effectiveness of our hierarchical memory system at retrieving the most relevant information across complex reasoning chains.</p>"},{"location":"legacy/advanced/performance/#near-full-context-performance","title":"Near Full-Context Performance","text":"<p>On single-hop and temporal tasks, MIRIX almost matches full-context performance while using only a small number of retrieved memory snippets. This validates our typed memory storage approach.</p>"},{"location":"legacy/advanced/performance/#exceptional-efficiency","title":"Exceptional Efficiency","text":"<p>In ScreenshotVQA, MIRIX achieves: - 59.50% overall accuracy (vs 44.10% for SigLIP@50) - 52.56MB average storage (vs 15.07GB for SigLIP@50) - 285x storage reduction compared to traditional approaches</p>"},{"location":"legacy/advanced/performance/#scalable-architecture","title":"Scalable Architecture","text":"<p>MIRIX's component-specific memory management scales efficiently across different data types and conversation lengths, maintaining performance as memory grows.</p>"},{"location":"legacy/advanced/performance/#implementation-details","title":"Implementation Details","text":""},{"location":"legacy/advanced/performance/#locomo-configuration","title":"LOCOMO Configuration","text":"<ul> <li>Backbone Model: GPT-4.1-mini (superior function calling: 29.75% vs 22.12% for GPT-4o-mini)</li> <li>Memory Architecture: Hierarchical multi-agent system with specialized memory managers</li> <li>Retrieval Strategy: Intelligent routing based on question type and memory content</li> </ul>"},{"location":"legacy/advanced/performance/#screenshotvqa-configuration","title":"ScreenshotVQA Configuration","text":"<ul> <li>Vision Model: Gemini-2.5-flash-preview-04-17</li> <li>Cloud Integration: Asynchronous image processing via Google Cloud</li> <li>Function Calls: 1-7 calls per processing step (1 for meta memory manager, 0-6 for specialized managers)</li> </ul>"},{"location":"legacy/advanced/performance/#comparative-analysis","title":"Comparative Analysis","text":"<p>vs. Traditional RAG Systems: MIRIX overcomes the global understanding limitations of simple RAG through its multi-agent memory architecture.</p> <p>vs. Full-Context Methods: While maintaining 97% of full-context performance, MIRIX uses dramatically less computational resources and storage.</p> <p>vs. Existing Memory Systems: MIRIX outperforms all tested memory systems (LangMem, Mem0, Zep, Memobase) across most evaluation categories.</p>"},{"location":"legacy/advanced/performance/#whats-next","title":"What's Next?","text":"<p>Return to explore other advanced features:</p> <p>Security &amp; Privacy \u2192 Backup &amp; Restore \u2192 </p>"},{"location":"legacy/advanced/security-privacy/","title":"Security &amp; Privacy","text":"<p>MIRIX is designed with privacy and security as core principles. All sensitive data processing happens locally, with user-controlled privacy settings and enterprise-grade security practices.</p>"},{"location":"legacy/advanced/security-privacy/#privacy-architecture","title":"Privacy Architecture","text":""},{"location":"legacy/advanced/security-privacy/#data-flow-overview","title":"Data Flow Overview","text":""},{"location":"legacy/advanced/security-privacy/#memory-updates-workflow","title":"Memory Updates Workflow","text":"<pre><code>graph TB\n    A[Screen Capture] --&gt; B[Temporary Cloud Storage&lt;br/&gt;Your Google Cloud]\n    B --&gt; C[Local Processing&lt;br/&gt;MIRIX Agents]\n    C --&gt; D[Local Database&lt;br/&gt;PostgreSQL/SQLite]\n\n    B --&gt; E[Auto-Delete&lt;br/&gt;After Processing]\n\n    style A fill:#e6f3ff\n    style B fill:#ffcccc\n    style C fill:#e6f3ff\n    style D fill:#ccffcc\n    style E fill:#ccffcc\n\n    classDef cloud fill:#ffcccc,stroke:#ff6666\n    classDef local fill:#ccffcc,stroke:#66cc66\n    classDef process fill:#e6f3ff,stroke:#3399ff\n\n    class B cloud\n    class D,E local\n    class A,C process</code></pre> <pre><code>graph TB\n    A[Screen Capture] --&gt; B[Temporary Cloud Storage&lt;br/&gt;Your Google Cloud]\n    B --&gt; C[Local Processing&lt;br/&gt;MIRIX Agents]\n    C --&gt; D[Local Database&lt;br/&gt;PostgreSQL/SQLite]\n\n    B --&gt; E[Auto-Delete&lt;br/&gt;After Processing]\n\n    style A fill:#4a90e2,stroke:#2c5282,color:#ffffff\n    style B fill:#e74c3c,stroke:#c0392b,color:#ffffff\n    style C fill:#4a90e2,stroke:#2c5282,color:#ffffff\n    style D fill:#27ae60,stroke:#1e8449,color:#ffffff\n    style E fill:#27ae60,stroke:#1e8449,color:#ffffff\n\n    classDef cloud fill:#e74c3c,stroke:#c0392b,color:#ffffff\n    classDef local fill:#27ae60,stroke:#1e8449,color:#ffffff\n    classDef process fill:#4a90e2,stroke:#2c5282,color:#ffffff\n\n    class B cloud\n    class D,E local\n    class A,C process</code></pre>"},{"location":"legacy/advanced/security-privacy/#query-response-workflow","title":"Query Response Workflow","text":"<pre><code>graph TB\n    F[User Query] --&gt; G[Local Memory&lt;br/&gt;Database]\n    F --&gt; H[Recent Screenshots&lt;br/&gt;Only Last 20 in RAM]\n    G --&gt; I[Gemini Processing&lt;br/&gt;Cloud API]\n    H --&gt; I\n    I --&gt; J[Local Response&lt;br/&gt;Generation]\n    J --&gt; K[Response to User]\n\n    L[Older Screenshots] --&gt; M[Automatically Deleted&lt;br/&gt;Not Stored Anywhere]\n\n    style F fill:#e6f3ff\n    style G fill:#ccffcc\n    style H fill:#ffffcc\n    style I fill:#ffcccc\n    style J fill:#e6f3ff\n    style K fill:#ccffcc\n    style L fill:#cccccc\n    style M fill:#cccccc\n\n    classDef cloud fill:#ffcccc,stroke:#ff6666\n    classDef local fill:#ccffcc,stroke:#66cc66\n    classDef process fill:#e6f3ff,stroke:#3399ff\n    classDef minimal fill:#ffffcc,stroke:#ffcc00\n    classDef deleted fill:#cccccc,stroke:#999999\n\n    class I cloud\n    class G,K local\n    class F,J process\n    class H minimal\n    class L,M deleted</code></pre> <pre><code>graph TB\n    F[User Query] --&gt; G[Local Memory&lt;br/&gt;Database]\n    F --&gt; H[Recent Screenshots&lt;br/&gt;Only Last 20 in RAM]\n    G --&gt; I[Gemini Processing&lt;br/&gt;Cloud API]\n    H --&gt; I\n    I --&gt; J[Local Response&lt;br/&gt;Generation]\n    J --&gt; K[Response to User]\n\n    L[Older Screenshots] --&gt; M[Automatically Deleted&lt;br/&gt;Not Stored Anywhere]\n\n    style F fill:#4a90e2,stroke:#2c5282,color:#ffffff\n    style G fill:#27ae60,stroke:#1e8449,color:#ffffff\n    style H fill:#f39c12,stroke:#d68910,color:#ffffff\n    style I fill:#e74c3c,stroke:#c0392b,color:#ffffff\n    style J fill:#4a90e2,stroke:#2c5282,color:#ffffff\n    style K fill:#27ae60,stroke:#1e8449,color:#ffffff\n    style L fill:#7f8c8d,stroke:#5d6d7e,color:#ffffff\n    style M fill:#7f8c8d,stroke:#5d6d7e,color:#ffffff\n\n    classDef cloud fill:#e74c3c,stroke:#c0392b,color:#ffffff\n    classDef local fill:#27ae60,stroke:#1e8449,color:#ffffff\n    classDef process fill:#4a90e2,stroke:#2c5282,color:#ffffff\n    classDef minimal fill:#f39c12,stroke:#d68910,color:#ffffff\n    classDef deleted fill:#7f8c8d,stroke:#5d6d7e,color:#ffffff\n\n    class I cloud\n    class G,K local\n    class F,J process\n    class H minimal\n    class L,M deleted</code></pre>"},{"location":"legacy/advanced/security-privacy/#privacy-principles","title":"Privacy Principles","text":"<ol> <li>Local Data Storage: All long-term user data remains on your local machine</li> <li>User-Controlled Cloud: Only your personal Google Cloud account is used for temporary storage</li> <li>Automatic Cleanup: Screenshots are automatically deleted after processing</li> <li>No Third-Party Sharing: Your data never leaves your control</li> <li>Transparent Processing: All data processing is documented and auditable</li> </ol>"},{"location":"legacy/advanced/security-privacy/#screenshot-handling","title":"Screenshot Handling","text":""},{"location":"legacy/advanced/security-privacy/#capture-process","title":"Capture Process","text":"<pre><code># MIRIX screenshot workflow\ndef screenshot_workflow():\n    # 1. Capture screenshot every second\n    screenshot = capture_screen()\n\n    # 2. Upload to YOUR Google Cloud storage\n    upload_to_user_cloud(screenshot, user_bucket)\n\n    # 3. Keep only recent 600 screenshots (~10 minutes)\n    maintain_recent_screenshots(limit=600)\n\n    # 4. Process screenshots through agents\n    process_with_agents(screenshot)\n\n    # 5. Delete processed screenshots\n    delete_processed_screenshots()\n</code></pre>"},{"location":"legacy/advanced/security-privacy/#whats-next","title":"What's Next?","text":"<p>Explore backup and restore features:</p> <p>Backup &amp; Restore \u2192 </p>"},{"location":"legacy/getting-started/installation/","title":"Installation &amp; Getting Started","text":"<p>This guide will walk you through setting up MIRIX on your system. There are two ways to install MIRIX:</p>"},{"location":"legacy/getting-started/installation/#choose-your-installation-method","title":"Choose Your Installation Method","text":""},{"location":"legacy/getting-started/installation/#quick-installation-recommended","title":"Quick Installation (Recommended)","text":"<p>One-click DMG installation - no coding required.</p> <p>Jump to Quick Installation \u2192</p>"},{"location":"legacy/getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For developers who want to contribute or customize MIRIX.</p> <p>Jump to Development Installation \u2192</p>"},{"location":"legacy/getting-started/installation/#quick-installation-dmg","title":"Quick Installation (DMG)","text":"<p>macOS Only</p> <p>The DMG installation is currently available for macOS only. Windows and Linux users should use the Development Installation method.</p>"},{"location":"legacy/getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS 10.15 or later</li> <li>A valid GEMINI API key</li> </ul> <p>API Key Required</p> <p>You'll need a GEMINI API key to use MIRIX. Get one free from Google AI Studio.</p>"},{"location":"legacy/getting-started/installation/#installation","title":"Installation","text":"<ol> <li> <p>Download the .dmg file below.</p> </li> <li> <p>Open it and drag MIRIX into your Applications folder.</p> </li> <li> <p>Launch the app.</p> </li> </ol> <p>Because this app is not notarized by Apple, you may see a security warning when launching it. To allow the app to run, you can either:</p> <ul> <li>Right-click (Control-click) the app in Applications and choose \"Open,\" then confirm in the dialog.</li> </ul> <p>or</p> <ul> <li>Run this command in Terminal to remove the quarantine attribute:   <pre><code>xattr -d com.apple.quarantine /Applications/MIRIX.app\n</code></pre></li> </ul> <p>This is expected behavior for open-source software distributed outside the App Store. The app is built directly from the code in this repository.</p>"},{"location":"legacy/getting-started/installation/#setup","title":"Setup","text":"<ol> <li>Enter your API key when prompted:</li> <li>Get your free API key from Google AI Studio</li> <li> <p>Paste it into the setup dialog</p> </li> <li> <p>Grant permissions when requested:</p> </li> <li>Screen recording permission (for screenshot capture)</li> </ol>"},{"location":"legacy/getting-started/installation/#start-using-mirix","title":"Start Using MIRIX","text":"<ol> <li>Start with chat - Initially, MIRIX functions as a chatbot where you can send messages</li> <li>Enable screen monitoring - Click the \"Screenshots\" tab, then click \"Start Monitoring\" to begin screen capture</li> <li>Let it learn - Give MIRIX a few minutes to build your initial memory base from your screen activities</li> <li>Ask questions - Once monitoring is active, interact with MIRIX about your recent activities and documents</li> </ol> <p>You're Ready!</p> <p>MIRIX is now installed and ready to use! It will automatically track your screen activities and build your personal memory base.</p>"},{"location":"legacy/getting-started/installation/#development-installation_1","title":"Development Installation","text":"<p>This method is for developers who want to set up MIRIX from source code.</p>"},{"location":"legacy/getting-started/installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Python 3.11 or later</li> <li>Node.js 14 or later (for the desktop app)</li> <li>A valid GEMINI API key</li> <li>PostgreSQL 17 (recommended for best performance)</li> </ul> <p>API Key Required</p> <p>You'll need a GEMINI API key to use MIRIX. Get one free from Google AI Studio.</p>"},{"location":"legacy/getting-started/installation/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/Mirix-AI/MIRIX.git\ncd MIRIX\n</code></pre>"},{"location":"legacy/getting-started/installation/#step-2-set-up-environment-variables","title":"Step 2: Set up Environment Variables","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># Create .env file\ntouch .env\n</code></pre> <p>Add your GEMINI API key to the <code>.env</code> file (you can get it for free at Google AI Studio):</p> <pre><code>GEMINI_API_KEY=your_api_key_here\n</code></pre>"},{"location":"legacy/getting-started/installation/#step-3-install-python-dependencies","title":"Step 3: Install Python Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"legacy/getting-started/installation/#step-4-database-setup","title":"Step 4: Database Setup","text":""},{"location":"legacy/getting-started/installation/#option-a-postgresql-recommended","title":"Option A: PostgreSQL (Recommended)","text":"<p>PostgreSQL provides better performance, scalability, and vector search capabilities.</p>"},{"location":"legacy/getting-started/installation/#install-postgresql-and-pgvector","title":"Install PostgreSQL and pgvector","text":"macOS (Homebrew)Ubuntu/DebianWindows <pre><code># Install PostgreSQL and pgvector\nbrew install postgresql@17 pgvector\n\n# Start PostgreSQL service\nbrew services start postgresql@17\n\n# Add PostgreSQL to your PATH\nexport PATH=\"$(brew --prefix postgresql@17)/bin:$PATH\"\n</code></pre> <pre><code># Install PostgreSQL\nsudo apt-get update\nsudo apt-get install postgresql postgresql-contrib\n\n# Install pgvector (see https://github.com/pgvector/pgvector for latest instructions)\n# Start PostgreSQL\nsudo systemctl start postgresql\n</code></pre> <ol> <li>Download PostgreSQL from postgresql.org</li> <li>Install pgvector following the official guide</li> <li>Start PostgreSQL service</li> </ol>"},{"location":"legacy/getting-started/installation/#create-database-and-enable-extensions","title":"Create Database and Enable Extensions","text":"<pre><code># Create the mirix database\ncreatedb mirix\n\n# Enable pgvector extension\npsql -U $(whoami) -d mirix -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n</code></pre>"},{"location":"legacy/getting-started/installation/#configure-environment-variables","title":"Configure Environment Variables","text":"<p>Add PostgreSQL configuration to your <code>.env</code> file:</p> <pre><code>GEMINI_API_KEY=your_api_key_here\n\n# PostgreSQL Configuration\n# Replace 'your_username' with your system username (find it with: echo $(whoami))\nMIRIX_PG_URI=postgresql+pg8000://your_username@localhost:5432/mirix\n</code></pre> <p>Username Setup</p> <p>This setup uses your system user for simplicity in development. For production, consider creating a dedicated PostgreSQL user with limited privileges.</p>"},{"location":"legacy/getting-started/installation/#option-b-sqlite-fallback","title":"Option B: SQLite (Fallback)","text":"<p>If PostgreSQL setup fails, MIRIX will automatically use SQLite. Simply omit the PostgreSQL environment variables from your <code>.env</code> file.</p> <p>SQLite Limitations</p> <p>SQLite works but has limitations in concurrent access and advanced search capabilities compared to PostgreSQL.</p>"},{"location":"legacy/getting-started/installation/#step-5-start-mirix-backend","title":"Step 5: Start MIRIX Backend","text":"<pre><code>python main.py\n</code></pre> <p>MIRIX will automatically create all necessary database tables on first startup and begin processing on-screen activities immediately.</p>"},{"location":"legacy/getting-started/installation/#step-6-launch-the-desktop-app","title":"Step 6: Launch the Desktop App","text":"<p>To use the graphical interface, you'll need to set up and run the frontend application.</p>"},{"location":"legacy/getting-started/installation/#prerequisites_2","title":"Prerequisites","text":"<p>Ensure you have Node.js (version 14 or later) installed:</p> <ul> <li>macOS: <code>brew install node</code></li> <li>Ubuntu/Debian: <code>sudo apt install nodejs npm</code></li> <li>Windows: Download from nodejs.org</li> </ul>"},{"location":"legacy/getting-started/installation/#setup-and-launch","title":"Setup and Launch","text":"<pre><code># Navigate to the frontend directory\ncd frontend\n\n# Install frontend dependencies\nnpm install\n\n# Launch the desktop application\nnpm run electron-dev\n</code></pre> <p>Two Windows Will Open</p> <p>This command will open two windows - this is normal behavior:</p> <ul> <li>Desktop App Window: The main MIRIX desktop application (an Electron app)</li> <li>Browser Window: A development server window that may open in your default browser</li> </ul> <p>You can safely close the browser window if it opens - the desktop app window is what you'll primarily use for interacting with MIRIX.</p> <p>The MIRIX desktop application will open, providing a user-friendly interface to interact with your personal assistant.</p> <p>Keep Both Running</p> <p>Keep both the backend (<code>python main.py</code>) and frontend (<code>npm run electron-dev</code>) running simultaneously for the full MIRIX experience.</p>"},{"location":"legacy/getting-started/installation/#getting-started","title":"Getting Started","text":"<p>Now that MIRIX is installed, you can start using it:</p> <ol> <li>Launch the desktop app - Either from Applications (DMG installation) or by running both backend and frontend (development installation)</li> <li>Start with chat - Initially, MIRIX functions as a chatbot where you can send messages</li> <li>Enable screen monitoring - Click the \"Screenshots\" tab on the right side of the app, then click \"Start Monitoring\" to begin screen capture</li> <li>Let it learn - Give MIRIX a few minutes to build your initial memory base from your screen activities</li> <li>Ask questions - Once monitoring is active, interact with MIRIX about your recent activities and documents</li> <li>Explore features - Check out the User Guide for advanced usage</li> </ol> <p>You're Ready!</p> <p>MIRIX is now tracking your screen activities and building your personal memory base.</p>"},{"location":"legacy/getting-started/installation/#what-happens-next","title":"What Happens Next?","text":"<ul> <li>Automatic Screenshot Capture: MIRIX takes and processes screenshots every 1.5 seconds</li> <li>Memory Building: Your activities are automatically categorized into 6 memory types</li> <li>Real-time Processing: 8 specialized agents work together to understand and organize your data</li> <li>Privacy First: All data is stored locally on your machine</li> </ul>"},{"location":"legacy/getting-started/installation/#explore-further","title":"Explore Further","text":"<ul> <li>Architecture Overview - Understand how MIRIX works</li> <li>User Guide - Learn advanced usage patterns</li> <li>Desktop App - Try the GUI interface</li> </ul>"},{"location":"legacy/getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"legacy/getting-started/installation/#dmg-installation-issues","title":"DMG Installation Issues","text":"\\\"MIRIX is damaged and can't be opened\\\" error <p>This is a common macOS Gatekeeper issue: <pre><code># Remove the quarantine attribute\nsudo xattr -rd com.apple.quarantine /Applications/MIRIX.app\n</code></pre></p> \\\"MIRIX can't be opened because it's from an unidentified developer\\\" <ol> <li>Go to System Preferences \u2192 Security &amp; Privacy \u2192 General</li> <li>Click \"Open Anyway\" next to the MIRIX message</li> <li>Or use the terminal command above</li> </ol> Screen recording permission denied <ol> <li>Go to System Preferences \u2192 Security &amp; Privacy \u2192 Screen Recording</li> <li>Check the box next to MIRIX</li> <li>Restart the application</li> </ol>"},{"location":"legacy/getting-started/installation/#development-installation-issues","title":"Development Installation Issues","text":""},{"location":"legacy/getting-started/installation/#common-postgresql-issues","title":"Common PostgreSQL Issues","text":"\\\"extension 'vector' is not available\\\" error <p>Install pgvector first: <pre><code># macOS\nbrew install pgvector\n\n# Then enable the extension\npsql -U $(whoami) -d mirix -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n</code></pre></p> \\\"permission denied to create extension\\\" error <p>The pgvector extension requires superuser privileges: <pre><code># Try connecting as postgres superuser\npsql -U postgres -d mirix -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n</code></pre></p> Connection refused or timeout <p>Make sure PostgreSQL service is running: <pre><code># macOS\nbrew services start postgresql@17\n\n# Linux\nsudo systemctl start postgresql\n</code></pre></p> \\\"database 'mirix' does not exist\\\" error <p>Create the database manually: <pre><code>createdb mirix\n\n# If createdb is not in PATH, use full path:\n# /opt/homebrew/opt/postgresql@17/bin/createdb mirix  # macOS with Homebrew\n</code></pre></p> \\\"pg_dump: No such file or directory\\\" error (during backup) <p>Set the correct PATH for PostgreSQL tools: <pre><code>export PATH=\"$(brew --prefix postgresql@17)/bin:$PATH\"\n</code></pre></p>"},{"location":"legacy/getting-started/installation/#frontend-issues","title":"Frontend Issues","text":"\\\"node: command not found\\\" error <p>Install Node.js first: <pre><code># macOS\nbrew install node\n\n# Ubuntu/Debian  \nsudo apt install nodejs npm\n\n# Verify installation\nnode --version\nnpm --version\n</code></pre></p> \\\"npm install\\\" fails with permission errors <p>Fix npm permissions or use a Node version manager: <pre><code># Option 1: Fix npm permissions (Linux/macOS)\nsudo chown -R $(whoami) ~/.npm\n\n# Option 2: Use nvm (recommended)\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash\nnvm install node\n</code></pre></p> \\\"Module not found\\\" errors when running electron <p>Ensure all dependencies are installed: <pre><code>cd frontend\nrm -rf node_modules package-lock.json\nnpm install\nnpm run electron-dev\n</code></pre></p>"},{"location":"legacy/user-guide/backend-usage/","title":"Backend Usage","text":"<p>Learn how to use the MIRIX backend directly through Python code for maximum flexibility and control over your personal assistant.</p>"},{"location":"legacy/user-guide/backend-usage/#getting-started","title":"Getting Started","text":""},{"location":"legacy/user-guide/backend-usage/#initialize-the-agent","title":"Initialize the Agent","text":"<p>First, create and initialize your MIRIX agent:</p> <pre><code>from mirix.agent import AgentWrapper\n\n# Initialize agent with configuration\nagent = AgentWrapper(\"./configs/mirix.yaml\")\n</code></pre> <p>Configuration File</p> <p>Make sure your <code>mirix.yaml</code> configuration file and <code>.env</code> file are properly set up before initializing the agent.</p> <p>Custom Models</p> <p>Want to use your own models? Check out our Custom Models Guide to learn how to serve models with vllm and integrate them with MIRIX.</p>"},{"location":"legacy/user-guide/backend-usage/#python-sdk-simplified-interface","title":"Python SDK (Simplified Interface)","text":"<p>For a simpler, more streamlined experience, you can use the MIRIX Python SDK. This approach is perfect for quick prototyping and straightforward use cases.</p>"},{"location":"legacy/user-guide/backend-usage/#quick-start-with-sdk","title":"Quick Start with SDK","text":"<pre><code>from mirix import Mirix\n\n# Initialize memory agent (defaults to Google Gemini 2.0 Flash)\nmemory_agent = Mirix(api_key=\"your-google-api-key\")\n\n# Add memories\nmemory_agent.add(\"The moon now has a president\")\nmemory_agent.add(\"John loves Italian food and is allergic to peanuts\")\n\n# Chat with memory context\nresponse = memory_agent.chat(\"Does the moon have a president?\")\nprint(response)  # \"Yes, according to my memory, the moon has a president.\"\n\nresponse = memory_agent.chat(\"What does John like to eat?\") \nprint(response)  # \"John loves Italian food. However, he's allergic to peanuts.\"\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#sdk-initialization-options","title":"SDK Initialization Options","text":"<p>The SDK supports various initialization parameters for different use cases:</p> <pre><code>from mirix import Mirix\n\n# Basic initialization (Google Gemini)\nagent = Mirix(api_key=\"your-google-api-key\")\n\n# Use OpenAI models\nagent = Mirix(\n    api_key=\"your-openai-api-key\",\n    model_provider=\"openai\",\n    model=\"gpt-4o\"\n)\n\n# Use Anthropic Claude\nagent = Mirix(\n    api_key=\"your-anthropic-api-key\", \n    model_provider=\"anthropic\",\n    model=\"claude-3-sonnet\"\n)\n\n# Use custom configuration file (default to `gemini-flash`)\nagent = Mirix(\n    api_key=\"your-gemini-api-key\",\n    config_path=\"./configs/mirix_custom_model.yaml\"\n)\n\n# Initialize with existing backup\nagent = Mirix(\n    api_key=\"your-api-key\",\n    load_from=\"./my_backup_directory\"\n)\n\n# Override model in default config\nagent = Mirix(\n    api_key=\"your-gemini-api-key\",\n    model=\"gemini-2.0-flash\"  # Override default model\n)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#supported-model-providers","title":"Supported Model Providers","text":"Provider <code>model_provider</code> Value Environment Variable Set Google Gemini <code>\"google_ai\"</code>, <code>\"google\"</code>, <code>\"gemini\"</code> <code>GEMINI_API_KEY</code> OpenAI <code>\"openai\"</code>, <code>\"gpt\"</code> <code>OPENAI_API_KEY</code> Anthropic <code>\"anthropic\"</code>, <code>\"claude\"</code> <code>ANTHROPIC_API_KEY</code> Custom Any custom string <code>{PROVIDER}_API_KEY</code>"},{"location":"legacy/user-guide/backend-usage/#sdk-usage-examples","title":"SDK Usage Examples","text":""},{"location":"legacy/user-guide/backend-usage/#building-a-personal-assistant","title":"Building a Personal Assistant","text":"<pre><code>from mirix import Mirix\n\n# Initialize with your API key\nassistant = Mirix(api_key=\"your-google-api-key\")\n\n# Store personal information\nassistant.add(\"I prefer coffee over tea\")\nassistant.add(\"My work hours are 9 AM to 5 PM\")\nassistant.add(\"Important meeting with client on Friday at 2 PM\")\n\n# Query your assistant\nresponse = assistant.chat(\"What's my schedule like this week?\")\nprint(response)\n\nresponse = assistant.chat(\"What drink do I prefer?\")\nprint(response)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#knowledge-base-building","title":"Knowledge Base Building","text":"<pre><code># Create a knowledge base about a project\nproject_memory = Mirix(api_key=\"your-google-api-key\")\n\n# Add project information\nproject_memory.add(\"Project name: MIRIX Documentation\")\nproject_memory.add(\"Tech stack: Python, MkDocs, Material theme\")\nproject_memory.add(\"Team members: Alice (PM), Bob (Dev), Carol (Designer)\")\nproject_memory.add(\"Deadline: End of December 2024\")\n\n# Query the knowledge base\nresponse = project_memory.chat(\"Who are the team members?\")\nprint(response)\n\nresponse = project_memory.chat(\"What technology are we using?\")\nprint(response)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#sdk-memory-management","title":"SDK Memory Management","text":""},{"location":"legacy/user-guide/backend-usage/#clearing-memory-and-conversation-history","title":"Clearing Memory and Conversation History","text":"<pre><code># Clear conversation history while keeping memories\nresult = agent.clear_conversation_history()\nif result['success']:\n    print(f\"Cleared {result['messages_deleted']} conversation messages\")\n    print(\"Memories are still preserved!\")\nelse:\n    print(f\"Failed: {result['error']}\")\n\n# Clear all memories (requires manual database reset)\nresult = agent.clear()\nif not result['success']:\n    print(result['warning'])\n    for instruction in result['instructions']:\n        print(instruction)\n    print(f\"Manual command: {result['manual_command']}\")\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#sdk-backup-and-restore","title":"SDK Backup and Restore","text":""},{"location":"legacy/user-guide/backend-usage/#save-current-state","title":"Save Current State","text":"<pre><code># Save with auto-generated timestamp directory\nresult = agent.save()\nif result['success']:\n    print(f\"Backup saved to: {result['path']}\")\n\n# Save to specific directory\nresult = agent.save(\"./my_project_backup\")\nif result['success']:\n    print(f\"Backup completed: {result['message']}\")\nelse:\n    print(f\"Backup failed: {result['error']}\")\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#load-from-backup","title":"Load from Backup","text":"<pre><code># Load state from backup\nresult = agent.load(\"./my_project_backup\")\nif result['success']:\n    print(\"Memory state restored successfully!\")\n    # All previous memories are now available\nelse:\n    print(f\"Restore failed: {result['error']}\")\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#advanced-sdk-features","title":"Advanced SDK Features","text":""},{"location":"legacy/user-guide/backend-usage/#using-agent-as-a-callable","title":"Using Agent as a Callable","text":"<pre><code># You can call the agent directly like a function\nagent = Mirix(api_key=\"your-api-key\")\nagent.add(\"Python is my favorite programming language\")\n\n# These are equivalent:\nresponse1 = agent.chat(\"What's my favorite language?\")\nresponse2 = agent(\"What's my favorite language?\")  # Callable interface\n\nprint(response1)  # Same as response2\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#chaining-operations","title":"Chaining Operations","text":"<pre><code># Initialize and set up memories in one flow\nagent = Mirix(api_key=\"your-api-key\")\n\n# Add multiple memories\nmemories = [\n    \"Team standup every Monday at 9 AM\",\n    \"Sprint planning on first Tuesday of each month\", \n    \"Code review sessions on Wednesday afternoons\",\n    \"Demo day every other Friday\"\n]\n\nfor memory in memories:\n    agent.add(memory)\n\n# Query the knowledge\nschedule = agent(\"What's our team meeting schedule?\")\nprint(schedule)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#basic-operations-agentwrapper","title":"Basic Operations (AgentWrapper)","text":""},{"location":"legacy/user-guide/backend-usage/#sending-messages","title":"Sending Messages","text":""},{"location":"legacy/user-guide/backend-usage/#simple-text-messages","title":"Simple Text Messages","text":"<pre><code># Send basic text information\nagent.send_message(\n    message=\"The moon now has a president.\",\n    memorizing=True,\n    force_absorb_content=True\n)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#multi-modal-content","title":"Multi-Modal Content","text":"<p>MIRIX can process text, images, and voice recordings together:</p> <pre><code># Send information with images and voice\nagent.send_message(\n    message=\"I'm working on a new project about machine learning.\",\n    image_uris=[\"/path/to/screenshot1.png\", \"/path/to/screenshot2.png\"],\n    memorizing=True,\n    force_absorb_content=True\n)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#structured-multi-modal-messages","title":"Structured Multi-Modal Messages","text":"<p>For complex multi-modal conversations:</p> <pre><code># Multi-modal message format\nagent.send_message(\n    message=[\n        {'type': 'text', 'text': \"The moon now has a president. This is how she looks like:\"},\n        {'type': 'image', 'image_url': \"base64_encoded_image\"}\n    ],\n    image_uris=[\"/path/to/image_1\", \"/path/to/image_2\"],\n    memorizing=True,\n    force_absorb_content=True\n)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#conversational-queries","title":"Conversational Queries","text":"<pre><code># Ask questions about your activities\nresponse = agent.send_message(\"What was I working on yesterday?\")\nprint(\"MIRIX:\", response)\n\n# Get specific information\nresponse = agent.send_message(\"Show me documents about PostgreSQL\")\nprint(\"MIRIX:\", response)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#understanding-parameters-agentwrapper","title":"Understanding Parameters (AgentWrapper)","text":""},{"location":"legacy/user-guide/backend-usage/#memory-management-control","title":"Memory Management Control","text":"<p>The <code>memorizing</code> parameter controls where messages are routed:</p> Value Destination Purpose <code>True</code> Meta-memory-manager Store information for long-term recall and knowledge building <code>False</code> Chat agent Direct conversation without memory storage <pre><code># Store information for future reference\nagent.send_message(\n    message=\"Project meeting notes - new feature requirements discussed\",\n    memorizing=True,\n    force_absorb_content=True\n)\n\n# Direct chat without memory storage\nagent.send_message(\n    message=\"What's the weather like today?\",\n    memorizing=False\n)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#content-absorption-control","title":"Content Absorption Control","text":"<p>The <code>force_absorb_content</code> parameter controls when information is processed:</p> Value Behavior Use Case <code>True</code> Process immediately Important information, real-time updates <code>False</code> Batch processing (after 20 images) Regular browsing, background activities <pre><code># Immediate processing for important content\nagent.send_message(\n    message=\"Critical meeting notes from client call\",\n    memorizing=True,\n    force_absorb_content=True\n)\n\n# Batch processing for regular activities\nagent.send_message(\n    message=\"Browsing documentation\",\n    memorizing=True,\n    force_absorb_content=False\n)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#input-types-and-formats","title":"Input Types and Formats","text":""},{"location":"legacy/user-guide/backend-usage/#message-parameter","title":"Message Parameter","text":"<p>Can be either a string or a structured list:</p> <p><pre><code># Simple string\nmessage = \"Working on the project documentation\"\n\n# Structured multi-modal format\nmessage = [\n    {'type': 'text', 'text': \"Here's the latest design mockup:\"},\n    {'type': 'image', 'image_url': \"data:image/png;base64,iVBORw0KGgoAAAANS...\"}\n]\n</code></pre> The images included here are in the user query. When the agent is generating responses, it will respond to the multi-modal query.  </p>"},{"location":"legacy/user-guide/backend-usage/#image-handling","title":"Image Handling","text":"<p>These are background images that will be treated as screenshots, the agent will not respond to these images, but rather extract information from them and save into memory.</p> <pre><code># Use local file paths\nimage_uris = [\n    \"/Users/username/Screenshots/screenshot1.png\",\n    \"/Users/username/Screenshots/screenshot2.png\",\n]\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#voice-files","title":"Voice Files","text":"<p>Under Robustness Test, Coming Soon.</p>"},{"location":"legacy/user-guide/backend-usage/#complete-workflow-examples-agentwrapper","title":"Complete Workflow Examples (AgentWrapper)","text":""},{"location":"legacy/user-guide/backend-usage/#example-1-daily-work-session","title":"Example 1: Daily Work Session","text":"<pre><code>from mirix.agent import AgentWrapper\n\n# Initialize\nagent = AgentWrapper(\"./configs/mirix.yaml\")\n\n# Morning briefing\nagent.send_message(\n    message=\"Starting work day. Today's priorities: finish documentation, review code, team meeting at 2 PM\",\n    memorizing=True,\n    force_absorb_content=True,\n)\n\n# Work activities (batch processing)\nagent.send_message(\n    message=\"Working on API documentation\",\n    image_uris=[\"/screenshots/vscode_api_docs.png\"],\n    memorizing=True,\n    force_absorb_content=False\n)\n\nagent.send_message(\n    message=\"Code review session with team\",\n    image_uris=[\"/screenshots/github_pr.png\"],\n    memorizing=True,\n    force_absorb_content=False\n)\n\n# Important meeting (immediate processing all existing accumulated messages)\nagent.send_message(\n    message=\"Team meeting - discussed Q4 roadmap, new feature priorities\",\n    voice_files=[\"base64_meeting_recording\"],\n    memorizing=True,\n    force_absorb_content=True\n)\n\n# End of day query\nresponse = agent.send_message(\"Summarize what I accomplished today\")\nprint(\"Today's Summary:\", response)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#example-2-research-session","title":"Example 2: Research Session","text":"<pre><code># Research topic introduction\nagent.send_message(\n    message=\"Researching machine learning optimization techniques\",\n    memorizing=True,\n    force_absorb_content=True\n)\n\n# Reading and capturing research materials\nresearch_screenshots = [\n    \"/screenshots/arxiv_paper1.png\",\n    \"/screenshots/arxiv_paper2.png\",\n    \"/screenshots/github_implementation.png\"\n]\n\nagent.send_message(\n    message=\"Reading papers on gradient descent optimization\",\n    image_uris=research_screenshots,\n    memorizing=True,\n    force_absorb_content=False\n)\n\n# Taking notes\nagent.send_message(\n    message=[\n        {'type': 'text', 'text': \"Key insight from the research:\"},\n        {'type': 'text', 'text': \"Adam optimizer performs better than SGD for sparse gradients\"}\n    ],\n    memorizing=True,\n    force_absorb_content=True\n)\n\n# Query research findings\nfindings = agent.send_message(\"What are the key points about optimization techniques I discovered?\")\nprint(\"Research Findings:\", findings)\n</code></pre>"},{"location":"legacy/user-guide/backend-usage/#whats-next","title":"What's Next?","text":"<p>Ready to explore more advanced topics?</p> <p>Custom Models \u2192</p>"},{"location":"legacy/user-guide/custom-models/","title":"Custom Models","text":"<p>Learn how to use custom models with MIRIX through vllm or other OpenAI-compatible API endpoints. This guide covers both end-user (Desktop App) and developer (Backend API) approaches.</p>"},{"location":"legacy/user-guide/custom-models/#overview","title":"Overview","text":"<p>MIRIX supports using custom models served through vllm or other OpenAI-compatible API endpoints. This allows you to:</p> <ul> <li>Use your own fine-tuned models</li> <li>Deploy models on your own infrastructure</li> <li>Integrate with alternative model providers</li> <li>Optimize for specific use cases and hardware</li> </ul> <p>Choose your approach:</p> <ul> <li>End-Users - Use the Desktop App's graphical interface</li> <li>Developers - Use configuration files and Python API</li> </ul>"},{"location":"legacy/user-guide/custom-models/#for-end-users-desktop-app","title":"For End-Users (Desktop App)","text":"<p>If you're using the MIRIX Desktop App, you can easily add custom models through the graphical interface without any configuration files.</p>"},{"location":"legacy/user-guide/custom-models/#step-1-set-up-vllm-model-server","title":"Step 1: Set Up vllm Model Server","text":"<p>First, serve your model using vllm. This step is the same for both end-users and developers:</p> <pre><code># Example: Serving Qwen3-32B model with multi-GPU setup\nCUDA_VISIBLE_DEVICES=0,1,2,3 \\\npython3 -m vllm.entrypoints.openai.api_server \\\n  --model Qwen/Qwen3-32B \\\n  --served-model-name qwen3-32b \\\n  --port 8001 \\\n  --tensor-parallel-size 4 \\\n  --max-model-len 32768 \\\n  --gpu-memory-utilization 0.9\n</code></pre> <p>vllm Parameters Explained:</p> Parameter Description Example Value <code>CUDA_VISIBLE_DEVICES</code> Specify which GPUs to use <code>0,1,2,3</code> <code>--model</code> HuggingFace model path or local model path <code>Qwen/Qwen3-32B</code> <code>--served-model-name</code> Custom name for the served model <code>qwen3-32b</code> <code>--port</code> Port to serve the API on <code>8001</code> <code>--tensor-parallel-size</code> Number of GPUs for tensor parallelism <code>4</code> <code>--max-model-len</code> Maximum sequence length <code>32768</code> <code>--gpu-memory-utilization</code> GPU memory usage ratio (0.0-1.0) <code>0.9</code>"},{"location":"legacy/user-guide/custom-models/#step-2-add-model-in-desktop-app","title":"Step 2: Add Model in Desktop App","text":"<p>Once your vllm server is running, add the model through the MIRIX Desktop App interface:</p> <p></p> <ol> <li>Open Settings - Navigate to Settings \u2192 Model Configuration</li> <li>Add Local Model - Click the \"Add\" button next to either Chat Agent Model or Memory Manager Model</li> <li>Fill in Model Details in the \"Add Local Model\" dialog:</li> </ol> Field Description Example Model Name The name identifier for your deployed model <code>qwen3-32b</code> Model Endpoint The API endpoint URL for your vllm server <code>http://localhost:8001/v1</code> Temperature Controls randomness in responses (0.0-2.0) <code>0.7</code> Max Tokens Maximum number of tokens to generate <code>4096</code> Maximum Length Maximum context length supported <code>32768</code> <ol> <li>Add Model - Click \"Add Model\" to save the configuration</li> <li>Select Model - Choose your newly added model from the dropdown</li> <li>Start Chatting - Your custom model is now ready to use!</li> </ol>"},{"location":"legacy/user-guide/custom-models/#ui-configuration-tips","title":"UI Configuration Tips","text":"<p>Model Endpoint Format</p> <p>Make sure your endpoint URL includes the <code>/v1</code> suffix: - \u2705 Correct: <code>http://localhost:8001/v1</code> - \u274c Incorrect: <code>http://localhost:8001</code></p> <p>Parameter Guidelines</p> <ul> <li>Temperature: Start with 0.7 for balanced responses</li> <li>Max Tokens: Match your typical response length needs</li> <li>Maximum Length: Should match your vllm <code>--max-model-len</code> setting</li> </ul>"},{"location":"legacy/user-guide/custom-models/#for-developers-backend-api","title":"For Developers (Backend API)","text":"<p>If you're using MIRIX programmatically through Python, you can configure custom models using YAML configuration files.</p>"},{"location":"legacy/user-guide/custom-models/#step-1-set-up-vllm-model-server_1","title":"Step 1: Set Up vllm Model Server","text":"<p>Use the same vllm server setup as shown in the End-Users section above.</p>"},{"location":"legacy/user-guide/custom-models/#step-2-create-custom-model-configuration","title":"Step 2: Create Custom Model Configuration","text":"<p>Create a custom configuration file in your <code>configs/</code> directory (e.g., <code>configs/mirix_custom_model.yaml</code>):</p> <pre><code>agent_name: mirix\nmodel_name: qwen3-32b\nmodel_endpoint: http://localhost:8001/v1\ngeneration_config:\n  temperature: 0.6\n  max_tokens: 4096\n  context_window: 32768\n</code></pre>"},{"location":"legacy/user-guide/custom-models/#step-3-configuration-parameters","title":"Step 3: Configuration Parameters","text":"Parameter Description Example <code>agent_name</code> Name identifier for your agent <code>mirix</code> <code>model_name</code> Model name as served by vllm <code>qwen3-32b</code> <code>model_endpoint</code> API endpoint URL <code>http://localhost:8001/v1</code> <code>system_prompt_folder</code> (Optional) Custom system prompt folder path <code>/path/to/system_prompt</code> <code>temperature</code> Controls response randomness (0.0-1.0) <code>0.6</code> <code>max_tokens</code> Maximum tokens in response <code>4096</code> <code>context_window</code> Maximum context length <code>32768</code>"},{"location":"legacy/user-guide/custom-models/#step-4-initialize-agent-with-custom-model","title":"Step 4: Initialize Agent with Custom Model","text":"<pre><code>from mirix.agent import AgentWrapper\n\n# Initialize agent with custom model configuration\nagent = AgentWrapper(\"./configs/mirix_custom_model.yaml\")\n</code></pre>"},{"location":"legacy/user-guide/custom-models/#supported-model-types","title":"Supported Model Types","text":"<p>MIRIX supports any model that provides an OpenAI-compatible API, including:</p> <ul> <li>Local Models - Models served locally with vllm, Ollama, or other servers</li> <li>Self-hosted Models - Models deployed on your own infrastructure  </li> <li>Custom Endpoints - Any OpenAI-compatible API endpoint</li> <li>Different Providers - Alternative model providers with OpenAI-compatible APIs</li> </ul>"},{"location":"legacy/user-guide/custom-models/#complete-example-developer-workflow","title":"Complete Example: Developer Workflow","text":"<p>Here's a complete end-to-end example for developers using the Python API:</p> <pre><code># 1. Start vllm server (in terminal)\n# CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -m vllm.entrypoints.openai.api_server \\\n#   --model Qwen/Qwen3-32B --served-model-name qwen3-32b --port 8001 \\\n#   --tensor-parallel-size 4 --max-model-len 32768 --gpu-memory-utilization 0.9\n\n# 2. Create configuration file\ncustom_config = \"\"\"\nagent_name: mirix_qwen\nmodel_name: qwen3-32b\nmodel_endpoint: http://localhost:8001/v1\ngeneration_config:\n  temperature: 0.7\n  max_tokens: 8192\n  context_window: 32768\n\"\"\"\n\n# Save to configs/mirix_custom_model.yaml\n\n# 3. Initialize and use\nfrom mirix.agent import AgentWrapper\n\nagent = AgentWrapper(\"./configs/mirix_custom_model.yaml\")\nresponse = agent.send_message(\"Hello! What model are you running on?\")\nprint(\"Custom Model Response:\", response)\n</code></pre>"},{"location":"legacy/user-guide/custom-models/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"legacy/user-guide/custom-models/#multiple-model-configurations","title":"Multiple Model Configurations","text":"<p>You can create different configuration files for different models:</p> <pre><code>configs/\n\u251c\u2500\u2500 mirix_qwen_32b.yaml      # Large model for complex tasks\n\u251c\u2500\u2500 mirix_qwen_7b.yaml       # Smaller model for quick responses\n\u2514\u2500\u2500 mirix_custom_local.yaml  # Local fine-tuned model\n</code></pre>"},{"location":"legacy/user-guide/custom-models/#model-specific-optimizations","title":"Model-Specific Optimizations","text":"<p>Different models may require different configurations:</p> <pre><code># For code generation models\nagent_name: mirix_code\nmodel_name: codellama-34b\nmodel_endpoint: http://localhost:8002/v1\ngeneration_config:\n  temperature: 0.1  # Lower temperature for code\n  max_tokens: 8192\n  context_window: 16384\n\n---\n# For creative writing models\nagent_name: mirix_creative\nmodel_name: llama2-70b\nmodel_endpoint: http://localhost:8003/v1\ngeneration_config:\n  temperature: 0.9  # Higher temperature for creativity\n  max_tokens: 4096\n  context_window: 32768\n</code></pre>"},{"location":"legacy/user-guide/custom-models/#custom-system-prompts","title":"Custom System Prompts","text":"<p>MIRIX allows you to customize the system prompts used by your agent by specifying a custom system prompt folder in your configuration file.</p>"},{"location":"legacy/user-guide/custom-models/#configuration","title":"Configuration","text":"<p>Add the <code>system_prompt_folder</code> parameter to your YAML configuration:</p> <pre><code>agent_name: mirix\nmodel_name: qwen3-32b\nmodel_endpoint: http://localhost:8001/v1\nsystem_prompt_folder: /path/to/system_prompt\ngeneration_config:\n  temperature: 0.6\n  max_tokens: 4096\n  context_window: 32768\n</code></pre>"},{"location":"legacy/user-guide/custom-models/#system-prompt-folder-structure","title":"System Prompt Folder Structure","text":"<p>The <code>system_prompt_folder</code> parameter should point to a directory containing your custom system prompt files. These prompts define how your agent behaves and responds.</p> <p>Available prompt files:</p> <pre><code>/path/to/system_prompt\n  \u251c\u2500\u2500 background_agent.txt        # Background processing agent prompt\n  \u251c\u2500\u2500 chat_agent.txt              # Main chat agent prompt\n  \u251c\u2500\u2500 core_memory_agent.txt       # Core memory management prompt\n  \u251c\u2500\u2500 episodic_memory_agent.txt   # Episodic memory prompt\n  \u251c\u2500\u2500 knowledge_vault_agent.txt   # Knowledge vault prompt\n  \u251c\u2500\u2500 meta_memory_agent.txt       # Meta memory manager prompt\n  \u251c\u2500\u2500 procedural_memory_agent.txt # Procedural memory prompt\n  \u251c\u2500\u2500 reflexion_agent.txt         # Reflexion agent prompt\n  \u251c\u2500\u2500 resource_memory_agent.txt   # Resource memory prompt\n  \u2514\u2500\u2500 semantic_memory_agent.txt   # Semantic memory prompt\n</code></pre> <p>Flexible Customization</p> <p>All prompt files are optional. You can customize only the agents you want to modify - MIRIX will use the default prompts for any files not found in your custom folder. This allows you to selectively override specific agent behaviors without needing to provide all 10 files.</p>"},{"location":"legacy/user-guide/custom-models/#example-configuration","title":"Example Configuration","text":"<pre><code>from mirix.agent import AgentWrapper\n\n# Initialize agent with custom system prompts\nagent = AgentWrapper(\"./configs/mirix_custom_prompts.yaml\")\n</code></pre> <p>Configuration file (<code>configs/mirix_custom_prompts.yaml</code>):</p> <pre><code>agent_name: mirix_debug\nmodel_name: gemini-2.0-flash\nsystem_prompt_folder: debug/base_debug  # Custom prompt folder\ngeneration_config:\n  temperature: 0.7\n  max_tokens: 4096\n  context_window: 8192\n</code></pre>"},{"location":"legacy/user-guide/custom-models/#troubleshooting","title":"Troubleshooting","text":""},{"location":"legacy/user-guide/custom-models/#common-issues","title":"Common Issues","text":"<p>Connection refused errors</p> <p>Check if your vllm server is running: <pre><code># Check if the port is in use\nlsof -i :8001\n\n# Check vllm server logs\ntail -f /path/to/vllm/logs\n</code></pre></p> <p>Out of memory errors</p> <p>Adjust GPU memory utilization: <pre><code># Reduce memory utilization\n--gpu-memory-utilization 0.8\n\n# Use smaller tensor parallel size\n--tensor-parallel-size 2\n</code></pre></p> <p>Model loading errors</p> <p>Verify model path and permissions: <pre><code># Check if model exists locally\nls -la /path/to/local/model/\n\n# Check HuggingFace cache\nls ~/.cache/huggingface/transformers/\n</code></pre></p>"},{"location":"legacy/user-guide/custom-models/#performance-optimization","title":"Performance Optimization","text":"<p>GPU Configuration</p> <p>For optimal performance: - Use all available GPUs with tensor parallelism - Set <code>gpu-memory-utilization</code> to 0.85-0.95 - Match <code>max-model-len</code> to your actual needs</p> <p>Memory Management</p> <p>Monitor system resources: <pre><code># Monitor GPU usage\nnvidia-smi -l 1\n\n# Monitor system memory\nhtop\n</code></pre></p>"},{"location":"legacy/user-guide/custom-models/#important-considerations","title":"Important Considerations","text":"<p>Model Compatibility</p> <p>Ensure your custom model is compatible with the tasks you want to perform. Some models may work better for chat, while others excel at specific tasks like code generation or analysis.</p> <p>Performance Considerations</p> <p>Local model performance depends on your hardware. Consider factors like: - GPU memory requirements - Model size vs. inference speed - Context window vs. memory usage - Network latency for remote endpoints</p> <p>Security</p> <p>When using custom endpoints: - Ensure secure connections (HTTPS) for remote servers - Validate API authentication if required - Monitor for unusual usage patterns</p>"},{"location":"legacy/user-guide/custom-models/#quick-reference","title":"Quick Reference","text":""},{"location":"legacy/user-guide/custom-models/#which-approach-should-i-use","title":"Which Approach Should I Use?","text":"Use Case Recommended Approach Why Regular MIRIX Desktop App user End-Users (Desktop App) Easy graphical interface, no configuration files needed Python developer building integrations Developers (Backend API) Programmatic control, configuration files, automation Both desktop and programmatic use Start with Desktop App, then use Backend API Best of both worlds"},{"location":"legacy/user-guide/custom-models/#essential-steps-both-approaches","title":"Essential Steps (Both Approaches)","text":"<ol> <li>Start vllm server with your model</li> <li>Configure in MIRIX (UI dialog OR YAML file)</li> <li>Start using your custom model</li> </ol>"},{"location":"legacy/user-guide/custom-models/#whats-next","title":"What's Next?","text":"<p>Ready to explore more MIRIX features?</p> <p>Multi-User Support \u2192 </p>"},{"location":"legacy/user-guide/custom-tools/","title":"Custom Tools","text":"<p>Create and manage custom tools in MIRIX to extend your agent's capabilities with your own Python functions. They will be run in a sandbox environment.</p>"},{"location":"legacy/user-guide/custom-tools/#basic-tool-creation","title":"Basic Tool Creation","text":""},{"location":"legacy/user-guide/custom-tools/#simple-function-tool","title":"Simple Function Tool","text":"<pre><code>from mirix import Mirix\n\n# Initialize memory agent\nmemory_agent = Mirix(api_key=\"your-google-api-key\")\n\n# Create a simple calculation tool\nresult = memory_agent.insert_tool(\n    name=\"calculate_sum\",\n    source_code=\"def calculate_sum(a: int, b: int) -&gt; int:\\n    return a + b\",\n    description=\"Calculate the sum of two numbers\",\n    args_info={\"a\": \"First number\", \"b\": \"Second number\"},\n    returns_info=\"The sum of a and b\",\n    tags=[\"math\", \"utility\"] # \"tags\" is optional and not important\n)\n</code></pre>"},{"location":"legacy/user-guide/custom-tools/#api-integration-tool","title":"API Integration Tool","text":"<pre><code># Create a weather API tool\nweather_tool_code = \"\"\"def get_weather(city: str, api_key: str) -&gt; str:\n    import requests\n    import json\n\n    url = f\"http://api.openweathermap.org/data/2.5/weather\"\n    params = {\n        'q': city,\n        'appid': api_key,\n        'units': 'metric'\n    }\n\n    try:\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        data = response.json()\n\n        temp = data['main']['temp']\n        description = data['weather'][0]['description']\n        humidity = data['main']['humidity']\n\n        return f\"Weather in {city}: {temp}\u00b0C, {description}, humidity: {humidity}%\"\n    except Exception as e:\n        return f\"Error getting weather data: {str(e)}\"\n\"\"\"\n\nresult = memory_agent.insert_tool(\n    name=\"get_weather\",\n    source_code=weather_tool_code,\n    description=\"Get current weather information for a specified city\",\n    args_info={\n        \"city\": \"Name of the city to get weather for\",\n        \"api_key\": \"OpenWeatherMap API key\"\n    },\n    returns_info=\"Weather information string with temperature, description, and humidity\",\n    tags=[\"weather\", \"api\", \"external\"] # \"tags\" is optional and not important\n)\n</code></pre>"},{"location":"legacy/user-guide/custom-tools/#tool-management-parameters","title":"Tool Management Parameters","text":""},{"location":"legacy/user-guide/custom-tools/#method-signature","title":"Method Signature","text":"<pre><code>def insert_tool(\n    self, \n    name: str, \n    source_code: str, \n    description: str, \n    args_info: Optional[Dict[str, str]] = None, \n    returns_info: Optional[str] = None, \n    tags: Optional[List[str]] = None, \n    apply_to_agents: Union[List[str], str] = 'all'\n) -&gt; Dict[str, Any]\n</code></pre>"},{"location":"legacy/user-guide/custom-tools/#parameter-details","title":"Parameter Details","text":"Parameter Type Description Example <code>name</code> <code>str</code> Function name identifier <code>\"calculate_sum\"</code> <code>source_code</code> <code>str</code> Python function code (without docstring) <code>\"def func(x): return x * 2\"</code> <code>description</code> <code>str</code> What the tool does <code>\"Calculate the sum of two numbers\"</code> <code>args_info</code> <code>Dict[str, str]</code> Argument descriptions <code>{\"a\": \"First number\", \"b\": \"Second number\"}</code> <code>returns_info</code> <code>str</code> Return value description <code>\"The sum of a and b\"</code> <code>tags</code> <code>List[str]</code> Categorization tags <code>[\"math\", \"utility\"]</code> <code>apply_to_agents</code> <code>Union[List[str], str]</code> Which agents get the tool <code>'all'</code> or <code>[\"chat_agent\", \"semantic_memory_agent\"]</code>"},{"location":"legacy/user-guide/custom-tools/#available-agent-names","title":"Available Agent Names","text":"<p>When specifying <code>apply_to_agents</code>, you can use any of these agent names:</p> <ul> <li><code>reflexion_agent</code> - Handles self-reflection and metacognitive processes</li> <li><code>core_memory_agent</code> - Manages core persistent memories</li> <li><code>episodic_memory_agent</code> - Handles episodic and experiential memories</li> <li><code>procedural_memory_agent</code> - Manages procedural knowledge and skills</li> <li><code>semantic_memory_agent</code> - Handles semantic knowledge and facts</li> <li><code>meta_memory_agent</code> - Manages meta-information about memories</li> <li><code>resource_memory_agent</code> - Handles resource and file associations</li> <li><code>knowledge_vault_agent</code> - Manages structured knowledge storage</li> <li><code>chat_agent</code> - Handles conversational interactions</li> <li><code>background_agent</code> - Processes background tasks and operations</li> </ul>"},{"location":"legacy/user-guide/desktop-app/","title":"Desktop App Guide","text":"<p>The MIRIX Desktop App provides a user-friendly graphical interface for interacting with your personal AI assistant and managing your digital memories. This comprehensive desktop application offers advanced features for chat interaction, screen monitoring, memory management, and data visualization.</p>"},{"location":"legacy/user-guide/desktop-app/#overview","title":"Overview","text":"<p>MIRIX Desktop is your central hub for AI-powered personal assistance, featuring:</p> <ul> <li>Interactive Chat Interface - Natural conversation with your AI assistant</li> <li>Screen Monitoring - Let your assistant observe and understand your screen activity</li> <li>Advanced Settings - Configure AI models, personas, and preferences</li> <li>Memory Visualization - Explore and understand your stored memories</li> <li>Data Management - Export and import your memory data</li> </ul>"},{"location":"legacy/user-guide/desktop-app/#chat-interface","title":"Chat Interface","text":"<p>The main chat window is where you interact with your MIRIX assistant. Features include:</p> <ul> <li>Real-time Conversations - Ask questions and get intelligent responses</li> <li>Context Awareness - Your assistant remembers previous conversations</li> <li>Multi-turn Dialogue - Engage in complex, ongoing discussions</li> <li>Helpful Responses - Get assistance with tasks, questions, and planning</li> </ul>"},{"location":"legacy/user-guide/desktop-app/#getting-started-with-chat","title":"Getting Started with Chat","text":"<ol> <li>Launch the MIRIX Desktop App</li> <li>Click the Chat tab to open the conversation interface</li> <li>Type your message in the input field at the bottom</li> <li>Press Enter or click Send to interact with your assistant</li> </ol>"},{"location":"legacy/user-guide/desktop-app/#screen-monitoring","title":"Screen Monitoring","text":"<p>The Screen Monitor feature allows your MIRIX assistant to observe your screen activity, providing contextual assistance based on what you're doing.</p>"},{"location":"legacy/user-guide/desktop-app/#features","title":"Features:","text":"<ul> <li>Real-time Screen Capture - Continuous monitoring of your desktop</li> <li>Smart Analysis - AI understanding of your current activities</li> <li>Contextual Assistance - Get help based on what's on your screen</li> <li>Privacy Controls - Full control over when monitoring is active</li> </ul>"},{"location":"legacy/user-guide/desktop-app/#how-to-use-screen-monitoring","title":"How to Use Screen Monitoring:","text":"<ol> <li>Navigate to the Screenshots tab</li> <li>Click Start Monitor to begin screen capture</li> <li>The status will show \"monitoring\" when active</li> <li>View captured screenshots and their timestamps</li> <li>Click Stop Monitor when you want to disable the feature</li> </ol> <p>Privacy Note: You have complete control over screen monitoring. The feature only captures screenshots when explicitly enabled and can be stopped at any time.</p>"},{"location":"legacy/user-guide/desktop-app/#settings-configuration","title":"Settings &amp; Configuration","text":"<p>Customize your MIRIX experience through the comprehensive Settings panel.</p>"},{"location":"legacy/user-guide/desktop-app/#model-configuration","title":"Model Configuration","text":"<p>Chat Agent Model:</p> <ul> <li>OpenAI Models - GPT-4, GPT-3.5-turbo, and other OpenAI models</li> <li>Gemini Models - Google's Gemini Pro and Flash models</li> <li>Choose the model that best fits your needs and preferences</li> </ul> <p>Memory Manager Model:</p> <ul> <li>Gemini Models Only - Currently supports Gemini models for memory operations</li> <li>Optimized for understanding and organizing your personal data</li> </ul>"},{"location":"legacy/user-guide/desktop-app/#persona-customization","title":"Persona Customization","text":"<p>Configure your assistant's personality and behavior:</p> <ul> <li>Template Selection - Choose from predefined persona templates</li> <li>Custom Persona - Write your own persona description</li> <li>Active Persona Display - See your current assistant's personality in the \"Agent's Active Persona (Core Memory)\" section</li> <li>Update Core Memory - Apply changes to your assistant's personality</li> </ul>"},{"location":"legacy/user-guide/desktop-app/#timezone-settings","title":"Timezone Settings","text":"<p>Set your local timezone to help your assistant understand:</p> <ul> <li>Time Context - When events occur in your local time</li> <li>Activity Patterns - Your daily routines and schedules  </li> <li>Temporal References - Accurate interpretation of time-based queries</li> </ul> <p>To Configure Timezone:</p> <ol> <li>Go to Settings \u2192 Preferences \u2192 Timezone</li> <li>Select your timezone from the dropdown menu</li> <li>Confirmation will appear when successfully set</li> </ol>"},{"location":"legacy/user-guide/desktop-app/#memory-management-visualization","title":"Memory Management &amp; Visualization","text":"<p>MIRIX builds and maintains different types of memories about your activities and interactions. The memory management interface lets you explore and understand how your assistant perceives and organizes information about you.</p>"},{"location":"legacy/user-guide/desktop-app/#memory-types","title":"Memory Types","text":"<ul> <li>\ud83d\udcda Episodic - Personal experiences and events from your life</li> <li>\ud83e\udde0 Semantic - Facts and general knowledge you've discussed</li> <li>\u2699\ufe0f Procedural - Skills, procedures, and how-to knowledge</li> <li>\ud83d\udcc1 Resource - Files, documents, and reference materials</li> <li>\u2b50 Core - Fundamental personality and preference information</li> <li>\ud83d\udd10 Credentials - Secure authentication and access information</li> </ul>"},{"location":"legacy/user-guide/desktop-app/#memory-views","title":"Memory Views","text":"<p>List View:</p> <ul> <li>Browse memories chronologically</li> <li>See memory details and timestamps</li> <li>Search through your memory collection</li> </ul> <p>Tree View:</p> <ul> <li>Visualize memory relationships and connections</li> <li>Understand how concepts link together</li> <li>Explore memory hierarchies and associations</li> </ul>"},{"location":"legacy/user-guide/desktop-app/#memory-search","title":"Memory Search","text":"<p>Use the search functionality to:</p> <ul> <li>Find specific memories quickly</li> <li>Filter by memory type or date</li> <li>Locate relevant information across your personal knowledge base</li> </ul>"},{"location":"legacy/user-guide/desktop-app/#data-export-import","title":"Data Export &amp; Import","text":"<p>MIRIX provides powerful options for managing your memory data.</p>"},{"location":"legacy/user-guide/desktop-app/#export-options","title":"Export Options","text":"<p>Excel Export:</p> <ul> <li>Export selected memory types to <code>.xlsx</code> format</li> <li>Separate sheets for each memory type</li> <li>Comprehensive data including timestamps and details</li> <li>Choose export location on your system</li> </ul> <p>Supported Export Formats:</p> <ul> <li>All memory types can be exported simultaneously</li> <li>Individual memory type selection available</li> <li>Structured data format for easy analysis</li> </ul>"},{"location":"legacy/user-guide/desktop-app/#import-options","title":"Import Options","text":"<p>Upload Memory Data:</p> <ul> <li>Import memories from external sources</li> <li>Mock upload functionality for testing</li> <li>Integrate data from other platforms or backups</li> </ul> <p>Agent Memory Platform:</p> <ul> <li>Connect to MIRIX's blockchain-based memory platform</li> <li>Secure, decentralized storage of your memories</li> <li>Cross-device synchronization capabilities</li> </ul>"},{"location":"legacy/user-guide/desktop-app/#how-to-export-your-memories","title":"How to Export Your Memories:","text":"<ol> <li>Click Existing Memory tab</li> <li>Select Upload &amp; Export button</li> <li>Choose which memory types to export (Episodic, Semantic, Procedural, Resource)</li> <li>Set your export file path or browse to select location</li> <li>Click Export Memories to generate your Excel file</li> </ol>"},{"location":"legacy/user-guide/desktop-app/#whats-next","title":"What's Next?","text":"<p>Ready to dive deeper? Explore these advanced topics:</p> <p>Backend Usage \u2192 Custom Models \u2192 </p>"},{"location":"legacy/user-guide/multi-user/","title":"Multi-User Support","text":"<p>MIRIX provides comprehensive multi-user support, allowing you to maintain separate memory contexts and conversations for different users or use cases. This is particularly useful for shared environments, family usage, or managing distinct projects with isolated memory contexts.</p>"},{"location":"legacy/user-guide/multi-user/#overview","title":"Overview","text":"<p>Multi-user functionality in MIRIX enables:</p> <ul> <li>Isolated Memory Contexts - Each user has their own private memory storage</li> <li>Separate Conversation Histories - Independent chat sessions per user</li> <li>Flexible User Management - Easy creation and switching between users</li> <li>Secure Isolation - User data remains completely separate and private</li> </ul> <p>Choose your approach:</p> <ul> <li>For End-Users - Use the Desktop App's graphical interface</li> <li>For Developers - Use the Python SDK for programmatic control</li> </ul>"},{"location":"legacy/user-guide/multi-user/#for-end-users-desktop-app","title":"For End-Users (Desktop App)","text":"<p>The MIRIX Desktop App provides an intuitive interface for managing multiple users without any coding required.</p>"},{"location":"legacy/user-guide/multi-user/#adding-a-new-user","title":"Adding a New User","text":"<ol> <li>Open User Management - Navigate to the User Selection section in the Settings</li> <li>Click Add User - Click the green \"+ Add User\" button</li> <li>Enter User Details in the \"Add New User\" dialog:</li> <li>User Name: Enter a unique identifier for the new user (e.g., \"Alice\", \"Work\", \"Personal\")</li> <li>Create User - Click \"Create User\" to add the new user to the system</li> <li>Select User - The new user will appear in the dropdown menu</li> </ol>"},{"location":"legacy/user-guide/multi-user/#switching-between-users","title":"Switching Between Users","text":"<p>Once you have multiple users created:</p> <ol> <li>Open Current User Dropdown - Click on the current user selection dropdown (shows \"default_user\" initially)</li> <li>Select Different User - Choose any user from the list</li> <li>Context Switch - The application will immediately switch to that user's memory context and conversation history</li> </ol>"},{"location":"legacy/user-guide/multi-user/#user-context-isolation","title":"User Context Isolation","text":"<p>Each user maintains completely separate:</p> <ul> <li>Memory Storage - Personal information, experiences, and knowledge</li> <li>Conversation History - Chat sessions and interactions</li> <li>Preferences - Settings and customizations</li> <li>File Associations - Documents and resources linked to memories</li> </ul> <p>User Privacy</p> <p>User contexts are completely isolated. Information added for one user will not be visible or accessible to other users, ensuring privacy and data separation.</p>"},{"location":"legacy/user-guide/multi-user/#use-cases-for-multiple-users","title":"Use Cases for Multiple Users","text":"<p>Family Sharing: <pre><code>- \"Dad\" - Work-related memories and conversations\n- \"Mom\" - Personal interests and activities  \n- \"Kids\" - Educational content and homework help\n</code></pre></p> <p>Work/Personal Separation: <pre><code>- \"Work\" - Professional projects and meetings\n- \"Personal\" - Hobbies, travel, and personal interests\n</code></pre></p> <p>Project Management: <pre><code>- \"Project_A\" - Specific project context and resources\n- \"Project_B\" - Different project with separate requirements\n- \"Research\" - Academic or research-focused activities\n</code></pre></p>"},{"location":"legacy/user-guide/multi-user/#for-developers-sdk-api","title":"For Developers (SDK &amp; API)","text":"<p>The MIRIX Python SDK provides full programmatic control over multi-user functionality for automated workflows and custom applications.</p>"},{"location":"legacy/user-guide/multi-user/#basic-multi-user-operations","title":"Basic Multi-User Operations","text":""},{"location":"legacy/user-guide/multi-user/#creating-and-managing-users","title":"Creating and Managing Users","text":"<pre><code>from mirix import Mirix\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Initialize memory agent\nmemory_agent = Mirix(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n# List existing users\nusers = memory_agent.list_users()\nprint(f\"Existing users: {[user.name for user in users]}\")\n\n# The default user is always available\ndefault_user = users[0]\nassert default_user.name == \"default_user\"\n\n# Create a new user\nalice = memory_agent.create_user(user_name=\"Alice\")\nprint(f\"Created user: {alice.name} with ID: {alice.id}\")\n\n# Create additional users\nbob = memory_agent.create_user(user_name=\"Bob\")\nwork_user = memory_agent.create_user(user_name=\"Work_Context\")\n</code></pre>"},{"location":"legacy/user-guide/multi-user/#user-specific-memory-operations","title":"User-Specific Memory Operations","text":"<pre><code># Add memories for specific users\nmemory_agent.add(\"John loves Italian food and is allergic to peanuts\", user_id=default_user.id)\nmemory_agent.add(\"John likes to eat British food\", user_id=alice.id)\nmemory_agent.add(\"Team meeting every Monday at 9 AM\", user_id=work_user.id)\n\n# Query memories from specific user contexts\nresponse_default = memory_agent.chat(\"What does John like to eat?\", user_id=default_user.id)\nprint(f\"Default user response: {response_default}\")\n# Output: \"John loves Italian food. However, he's allergic to peanuts.\"\n\nresponse_alice = memory_agent.chat(\"What does John like to eat?\", user_id=alice.id)\nprint(f\"Alice's response: {response_alice}\")\n# Output: \"John likes to eat British food.\"\n\nresponse_work = memory_agent.chat(\"When is our team meeting?\", user_id=work_user.id)\nprint(f\"Work context response: {response_work}\")\n# Output: \"The team meeting is every Monday at 9 AM.\"\n</code></pre>"},{"location":"legacy/user-guide/multi-user/#sdk-user-management","title":"SDK User Management","text":""},{"location":"legacy/user-guide/multi-user/#complete-user-lifecycle","title":"Complete User Lifecycle","text":"<pre><code>from mirix import Mirix\n\n# Initialize with environment variables\nmemory_agent = Mirix(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n# Create users for different contexts\nproject_user = memory_agent.create_user(user_name=\"ProjectX\")\npersonal_user = memory_agent.create_user(user_name=\"Personal\")\n\n# Set up project-specific memories\nproject_memories = [\n    \"Project X uses React and Node.js\",\n    \"Database is PostgreSQL hosted on AWS\",\n    \"Team lead is Sarah Johnson\",\n    \"Sprint review every Friday at 3 PM\"\n]\n\nfor memory in project_memories:\n    memory_agent.add(memory, user_id=project_user.id)\n\n# Set up personal memories  \npersonal_memories = [\n    \"Favorite restaurant is Chez Laurent\",\n    \"Gym membership at 24 Hour Fitness\",\n    \"Doctor appointment next Tuesday\",\n    \"Planning vacation to Japan in spring\"\n]\n\nfor memory in personal_memories:\n    memory_agent.add(memory, user_id=personal_user.id)\n\n# Use different contexts appropriately\nwork_query = memory_agent.chat(\"What technology stack are we using?\", user_id=project_user.id)\npersonal_query = memory_agent.chat(\"Where should I go for dinner?\", user_id=personal_user.id)\n\nprint(f\"Work context: {work_query}\")\nprint(f\"Personal context: {personal_query}\")\n</code></pre>"},{"location":"legacy/user-guide/multi-user/#whats-next","title":"What's Next?","text":"<p>Ready to dive deeper into MIRIX development?</p> <p>Performance \u2192</p>"},{"location":"memory-search/search/","title":"Memory Search","text":"<p>Mirix provides powerful search capabilities across all memory types. Choose between direct search with <code>search()</code>, context-aware retrieval with <code>retrieve_with_conversation()</code>, or topic-based retrieval with <code>retrieve_with_topic()</code>.</p>"},{"location":"memory-search/search/#search-methods-overview","title":"Search Methods Overview","text":"Method Use Case Key Features <code>search()</code> Direct keyword/semantic search BM25 or embedding search, field-specific, temporal filters <code>retrieve_with_conversation()</code> Context-aware memory retrieval Analyzes conversation, extracts topics, retrieves relevant memories <code>retrieve_with_topic()</code> Topic-based memory lookup Simple topic/keyword search across all memory types <code>search_all_users()</code> Organization-wide search Search across all users in an organization"},{"location":"memory-search/search/#direct-search","title":"Direct Search","text":"<p>The <code>search()</code> method provides fine-grained control over memory queries.</p>"},{"location":"memory-search/search/#basic-search","title":"Basic Search","text":"<pre><code>from mirix import MirixClient\n\nclient = MirixClient(api_key=\"your_api_key_here\")\nclient.initialize_meta_agent(provider=\"openai\")\n\n# Search across all memory types\nresults = client.search(\n    user_id=\"demo-user\",\n    query=\"restaurants\",\n    limit=5,\n)\n\nprint(f\"Found {results['count']} results\")\nfor result in results['results']:\n    print(f\"- [{result['memory_type']}] {result.get('summary', result.get('caption', 'N/A'))}\")\n</code></pre>"},{"location":"memory-search/search/#search-specific-memory-types","title":"Search Specific Memory Types","text":"<pre><code># Search only episodic memories\nepisodic_results = client.search(\n    user_id=\"demo-user\",\n    query=\"meeting\",\n    memory_type=\"episodic\",\n    limit=10,\n)\n\n# Search procedural memories\nprocedural_results = client.search(\n    user_id=\"demo-user\",\n    query=\"deployment process\",\n    memory_type=\"procedural\",\n    limit=5,\n)\n</code></pre>"},{"location":"memory-search/search/#search-specific-fields","title":"Search Specific Fields","text":"<p>Different memory types have different searchable fields:</p> <pre><code># Search in episodic memory details\nepisodic_results = client.search(\n    user_id=\"demo-user\",\n    query=\"database migration\",\n    memory_type=\"episodic\",\n    search_field=\"details\",  # Options: \"summary\", \"details\"\n    limit=10,\n)\n\n# Search in resource memory content\nresource_results = client.search(\n    user_id=\"demo-user\",\n    query=\"API documentation\",\n    memory_type=\"resource\",\n    search_field=\"content\",  # Options: \"summary\", \"content\"\n    limit=5,\n)\n\n# Search in semantic memory\nsemantic_results = client.search(\n    user_id=\"demo-user\",\n    query=\"Python libraries\",\n    memory_type=\"semantic\",\n    search_field=\"details\",  # Options: \"name\", \"summary\", \"details\"\n    limit=5,\n)\n</code></pre>"},{"location":"memory-search/search/#searchable-fields-by-memory-type","title":"Searchable Fields by Memory Type","text":"Memory Type Available Fields <code>episodic</code> <code>summary</code>, <code>details</code> <code>resource</code> <code>summary</code>, <code>content</code> <code>procedural</code> <code>summary</code>, <code>steps</code> <code>knowledge</code> <code>caption</code>, <code>secret_value</code> <code>semantic</code> <code>name</code>, <code>summary</code>, <code>details</code> <code>all</code> Use <code>\"null\"</code> (searches all default fields)"},{"location":"memory-search/search/#search-methods-bm25-vs-embedding","title":"Search Methods: BM25 vs Embedding","text":""},{"location":"memory-search/search/#bm25-search-keyword-based","title":"BM25 Search (Keyword-based)","text":"<p>BM25 is a keyword-based search algorithm that works well for exact matches and specific terms:</p> <pre><code># Fast keyword search\nresults = client.search(\n    user_id=\"demo-user\",\n    query=\"PostgreSQL performance optimization\",\n    search_method=\"bm25\",  # Default\n    limit=10,\n)\n</code></pre> <p>Best for: - Exact keyword matches - Technical terms and identifiers - Fast retrieval - No embedding model required</p>"},{"location":"memory-search/search/#embedding-search-semantic","title":"Embedding Search (Semantic)","text":"<p>Embedding search finds semantically similar memories, even if they don't share exact keywords:</p> <pre><code># Semantic similarity search\nresults = client.search(\n    user_id=\"demo-user\",\n    query=\"How do I improve database speed?\",\n    search_method=\"embedding\",\n    similarity_threshold=0.7,  # Optional: filter by similarity\n    limit=10,\n)\n</code></pre> <p>Best for: - Conceptual similarity - Natural language queries - Finding related topics - Understanding context</p> <p>Similarity Thresholds</p> <p>When using <code>search_method=\"embedding\"</code>, you can filter results by similarity:</p> <ul> <li><code>0.5</code> - Strict: Only highly relevant results</li> <li><code>0.7</code> - Moderate: Reasonably relevant results (recommended)</li> <li><code>0.9</code> - Loose: Loosely related results</li> <li><code>None</code> - No filtering: Returns top N results regardless of similarity</li> </ul>"},{"location":"memory-search/search/#temporal-filtering","title":"Temporal Filtering","text":"<p>Filter episodic memories by time range:</p> <pre><code># Find memories from a specific date range\ntemporal_results = client.search(\n    user_id=\"demo-user\",\n    query=\"project updates\",\n    memory_type=\"episodic\",  # Temporal filters only apply to episodic\n    start_date=\"2025-12-01T00:00:00\",\n    end_date=\"2025-12-05T23:59:59\",\n    limit=10,\n)\n</code></pre> <p>Date format: ISO 8601 (<code>YYYY-MM-DDTHH:MM:SS</code> or <code>YYYY-MM-DDTHH:MM:SSZ</code>)</p>"},{"location":"memory-search/search/#tag-filtering","title":"Tag Filtering","text":"<p>Use custom tags to organize and filter memories:</p> <pre><code># Filter by tags\ntagged_results = client.search(\n    user_id=\"demo-user\",\n    query=\"customer support\",\n    filter_tags={\n        \"project\": \"alpha\",\n        \"department\": \"engineering\",\n        \"priority\": \"high\"\n    },\n    limit=10,\n)\n</code></pre> <p>Tags are set when adding memories:</p> <pre><code>client.add(\n    user_id=\"demo-user\",\n    messages=[\n        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Fix the login bug\"}]},\n        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"I'll prioritize that.\"}]},\n    ],\n    filter_tags={\n        \"project\": \"alpha\",\n        \"department\": \"engineering\",\n        \"priority\": \"high\"\n    }\n)\n</code></pre>"},{"location":"memory-search/search/#context-aware-retrieval","title":"Context-Aware Retrieval","text":"<p>The <code>retrieve_with_conversation()</code> method analyzes conversation context to retrieve relevant memories intelligently:</p> <pre><code># Retrieve memories based on conversation context\nmemories = client.retrieve_with_conversation(\n    user_id=\"demo-user\",\n    messages=[\n        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What did we discuss about the database migration last week?\"}]},\n    ],\n    limit=5,\n)\n\n# Access memories by type\nif memories.get(\"memories\"):\n    episodic = memories[\"memories\"].get(\"episodic\", {})\n    semantic = memories[\"memories\"].get(\"semantic\", {})\n\n    print(f\"Found {episodic.get('total_count', 0)} episodic memories\")\n    print(f\"Found {semantic.get('total_count', 0)} semantic memories\")\n</code></pre>"},{"location":"memory-search/search/#automatic-temporal-extraction","title":"Automatic Temporal Extraction","text":"<p><code>retrieve_with_conversation()</code> automatically extracts time references from queries:</p> <pre><code># Natural language time references\nmemories = client.retrieve_with_conversation(\n    user_id=\"demo-user\",\n    messages=[\n        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What happened yesterday?\"}]},\n    ],\n    limit=5,\n)\n\n# Check what time range was detected\nif memories.get(\"temporal_expression\"):\n    print(f\"Detected time: {memories['temporal_expression']}\")\n    print(f\"Date range: {memories.get('date_range')}\")\n</code></pre> <p>Supported expressions: \"today\", \"yesterday\", \"last week\", \"last 3 days\", \"this month\", etc.</p>"},{"location":"memory-search/search/#explicit-date-ranges","title":"Explicit Date Ranges","text":"<p>You can also provide explicit date ranges:</p> <pre><code>memories = client.retrieve_with_conversation(\n    user_id=\"demo-user\",\n    messages=[\n        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What did we work on?\"}]},\n    ],\n    start_date=\"2025-12-01T00:00:00\",\n    end_date=\"2025-12-05T23:59:59\",\n    limit=5,\n)\n</code></pre>"},{"location":"memory-search/search/#topic-based-retrieval","title":"Topic-Based Retrieval","text":"<p>Search by topic or keyword across all memory types:</p> <pre><code># Simple topic search\nmemories = client.retrieve_with_topic(\n    user_id=\"demo-user\",\n    topic=\"machine learning\",\n    limit=5,\n    filter_tags={\"project\": \"ai-research\"}\n)\n\n# Check results\nfor memory_type, data in memories.get(\"memories\", {}).items():\n    if data and data.get(\"total_count\", 0) &gt; 0:\n        print(f\"{memory_type}: {data['total_count']} results\")\n</code></pre>"},{"location":"memory-search/search/#organization-wide-search","title":"Organization-Wide Search","text":"<p>Search across all users in your organization (requires appropriate permissions):</p> <pre><code># Search memories across all users\norg_results = client.search_all_users(\n    query=\"quarterly planning\",\n    memory_type=\"episodic\",\n    search_method=\"embedding\",\n    limit=20,\n    client_id=client.client_id,  # Optional: scope to specific client\n    similarity_threshold=0.7\n)\n\nprint(f\"Found {org_results['count']} results across all users\")\n</code></pre> <p>Privacy &amp; Permissions</p> <p>Organization-wide search respects user privacy settings and client scopes. You can only search memories that your API key has permission to access.</p>"},{"location":"memory-search/search/#complete-parameter-reference","title":"Complete Parameter Reference","text":""},{"location":"memory-search/search/#search-parameters","title":"<code>search()</code> Parameters","text":"Parameter Type Default Description <code>user_id</code> str Required User ID for the search <code>query</code> str Required Search query string <code>memory_type</code> str <code>\"all\"</code> Memory type: <code>episodic</code>, <code>resource</code>, <code>procedural</code>, <code>knowledge</code>, <code>semantic</code>, or <code>all</code> <code>search_field</code> str <code>\"null\"</code> Specific field to search (see table above) <code>search_method</code> str <code>\"bm25\"</code> Search method: <code>bm25</code> or <code>embedding</code> <code>limit</code> int <code>10</code> Maximum number of results per memory type <code>filter_tags</code> dict <code>None</code> Optional tags for filtering <code>similarity_threshold</code> float <code>None</code> Similarity threshold for embedding search (0.0-2.0) <code>start_date</code> str <code>None</code> Start date for temporal filtering (ISO 8601) <code>end_date</code> str <code>None</code> End date for temporal filtering (ISO 8601)"},{"location":"memory-search/search/#retrieve_with_conversation-parameters","title":"<code>retrieve_with_conversation()</code> Parameters","text":"Parameter Type Default Description <code>user_id</code> str Required User ID for retrieval <code>messages</code> List Required Conversation messages (must end with user turn) <code>limit</code> int <code>10</code> Maximum items per memory type <code>filter_tags</code> dict <code>None</code> Optional tags for filtering <code>use_cache</code> bool <code>True</code> Enable Redis caching <code>start_date</code> str <code>None</code> Override automatic temporal extraction <code>end_date</code> str <code>None</code> Override automatic temporal extraction"},{"location":"memory-search/search/#retrieve_with_topic-parameters","title":"<code>retrieve_with_topic()</code> Parameters","text":"Parameter Type Default Description <code>user_id</code> str Required User ID for retrieval <code>topic</code> str Required Topic or keyword to search <code>limit</code> int <code>10</code> Maximum items per memory type <code>filter_tags</code> dict <code>None</code> Optional tags for filtering <code>use_cache</code> bool <code>True</code> Enable Redis caching"},{"location":"memory-search/search/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the right method:</li> <li>Use <code>search()</code> for direct, specific queries</li> <li>Use <code>retrieve_with_conversation()</code> for conversational context</li> <li> <p>Use <code>retrieve_with_topic()</code> for simple keyword lookups</p> </li> <li> <p>Optimize search method:</p> </li> <li>Use <code>bm25</code> for exact keyword matches (faster)</li> <li> <p>Use <code>embedding</code> for semantic similarity (more flexible)</p> </li> <li> <p>Leverage temporal filtering:</p> </li> <li>Combine search with date ranges for time-specific queries</li> <li> <p>Use natural language time expressions with <code>retrieve_with_conversation()</code></p> </li> <li> <p>Use tags effectively:</p> </li> <li>Tag memories during <code>add()</code> operations</li> <li>Filter searches by relevant tags</li> <li> <p>Use hierarchical tags (e.g., <code>project:module:feature</code>)</p> </li> <li> <p>Tune similarity thresholds:</p> </li> <li>Start with <code>0.7</code> for embedding search</li> <li>Lower for stricter, higher for broader results</li> <li>Monitor result quality and adjust</li> </ol> <p> Memory Write Next: Architecture </p>"},{"location":"memory-write/configuration/","title":"Configuration","text":"<p>Mirix uses a YAML configuration to define LLM settings, embeddings, and the meta-agent layout. This guide covers all configuration options and provides examples for different LLM providers.</p>"},{"location":"memory-write/configuration/#quick-start-using-provider-shortcuts","title":"Quick Start: Using Provider Shortcuts","text":"<p>The easiest way to initialize Mirix is using provider shortcuts:</p> <pre><code>from mirix import MirixClient\n\nclient = MirixClient(api_key=\"your_api_key_here\")\n\n# Simple initialization - uses default config for the provider\nclient.initialize_meta_agent(provider=\"openai\")  # or \"anthropic\", \"google_ai\"\n</code></pre> <p>This automatically configures the appropriate model, embeddings, and memory agents for the provider.</p>"},{"location":"memory-write/configuration/#advanced-custom-yaml-configuration","title":"Advanced: Custom YAML Configuration","text":"<p>For full control, create a YAML configuration file. Mirix includes examples for multiple providers in <code>mirix/configs/examples/</code>.</p> OpenAI (Recommended)Google GeminiAnthropic Claude <pre><code>llm_config:\n  model: \"gpt-4o-mini\"\n  model_endpoint_type: \"openai\"\n  api_key: \"sk-proj-xxx\"  # Or use OPENAI_API_KEY env var\n  model_endpoint: \"https://api.openai.com/v1\"\n  context_window: 128000\n\n# Optional: separate model for topic extraction\ntopic_extraction_llm_config:\n  model: \"gpt-4o-nano\"\n  model_endpoint_type: \"openai\"\n  api_key: \"sk-proj-xxx\"\n  model_endpoint: \"https://api.openai.com/v1\"\n  context_window: 128000\n\nbuild_embeddings_for_memory: true\n\nembedding_config:\n  embedding_model: \"text-embedding-3-small\"\n  embedding_endpoint: \"https://api.openai.com/v1\"\n  api_key: \"sk-proj-xxx\"\n  embedding_endpoint_type: \"openai\"\n  embedding_dim: 1536\n\nmeta_agent_config:\n  system_prompts_folder: mirix\\prompts\\system\\base\n  agents:\n    - core_memory_agent\n    - resource_memory_agent\n    - semantic_memory_agent\n    - episodic_memory_agent\n    - procedural_memory_agent\n    - knowledge_memory_agent\n    - reflexion_agent\n    - background_agent\n  memory:\n    core:\n      - label: \"human\"\n        value: \"\"\n      - label: \"persona\"\n        value: \"I am a helpful assistant.\"\n    decay:\n      fade_after_days: 30      # Memories become inactive (excluded from retrieval)\n      expire_after_days: 90    # Memories are permanently deleted\n</code></pre> <pre><code>llm_config:\n  model: \"gemini-2.0-flash\"\n  model_endpoint_type: \"google_ai\"\n  model_endpoint: \"https://generativelanguage.googleapis.com\"\n  context_window: 1000000  # 1M context window!\n\ntopic_extraction_llm_config:\n  model: \"gemini-2.0-flash-lite\"\n  model_endpoint_type: \"google_ai\"\n  model_endpoint: \"https://generativelanguage.googleapis.com\"\n  context_window: 1000000\n\nbuild_embeddings_for_memory: true\n\nembedding_config:\n  embedding_model: \"text-embedding-004\"\n  embedding_endpoint_type: \"google_ai\"\n  embedding_endpoint: \"https://generativelanguage.googleapis.com\"\n  embedding_dim: 768\n\nmeta_agent_config:\n  agents:\n    - core_memory_agent\n    - resource_memory_agent\n    - semantic_memory_agent\n    - episodic_memory_agent\n    - procedural_memory_agent\n    - knowledge_memory_agent\n    - reflexion_agent\n    - background_agent\n  memory:\n    core:\n      - label: \"human\"\n        value: \"\"\n      - label: \"persona\"\n        value: \"I am a helpful assistant.\"\n    decay:\n      fade_after_days: 30\n      expire_after_days: 90\n</code></pre> <pre><code>llm_config:\n  model: \"claude-3-5-sonnet-20241022\"\n  model_endpoint_type: \"anthropic\"\n  model_endpoint: \"https://api.anthropic.com/v1\"\n  context_window: 200000\n\ntopic_extraction_llm_config:\n  model: \"claude-3-5-sonnet-20241022\"\n  model_endpoint_type: \"anthropic\"\n  model_endpoint: \"https://api.anthropic.com/v1\"\n  context_window: 200000\n\nbuild_embeddings_for_memory: true\n\n# Note: Anthropic doesn't provide embeddings, use Google or OpenAI\nembedding_config:\n  embedding_model: \"text-embedding-004\"\n  embedding_endpoint_type: \"google_ai\"\n  embedding_endpoint: \"https://generativelanguage.googleapis.com\"\n  embedding_dim: 768\n\nmeta_agent_config:\n  agents:\n    - core_memory_agent\n    - resource_memory_agent\n    - semantic_memory_agent\n    - episodic_memory_agent\n    - procedural_memory_agent\n    - knowledge_memory_agent\n    - reflexion_agent\n    - background_agent\n  memory:\n    core:\n      - label: \"human\"\n        value: \"\"\n      - label: \"persona\"\n        value: \"I am a helpful assistant.\"\n    decay:\n      fade_after_days: 30\n      expire_after_days: 90\n</code></pre>"},{"location":"memory-write/configuration/#configuration-fields-explained","title":"Configuration Fields Explained","text":""},{"location":"memory-write/configuration/#llm-configuration","title":"LLM Configuration","text":"Field Description Required <code>model</code> Model name (e.g., \"gpt-4o-mini\", \"claude-3-5-sonnet\") Yes <code>model_endpoint_type</code> Provider type: <code>openai</code>, <code>anthropic</code>, <code>google_ai</code> Yes <code>model_endpoint</code> API endpoint URL Yes <code>api_key</code> API key (can use environment variable instead) Depends on provider <code>context_window</code> Maximum context window size Yes"},{"location":"memory-write/configuration/#topic-extraction-llm-optional","title":"Topic Extraction LLM (Optional)","text":"<p>Configure a separate (often cheaper/faster) model for topic extraction during retrieval. If omitted, uses the main LLM.</p>"},{"location":"memory-write/configuration/#embedding-configuration","title":"Embedding Configuration","text":"Field Description Required <code>embedding_model</code> Embedding model name Yes <code>embedding_endpoint_type</code> Provider type for embeddings Yes <code>embedding_endpoint</code> Embedding API endpoint Yes <code>api_key</code> API key for embedding service Depends on provider <code>embedding_dim</code> Embedding dimension (e.g., 1536, 768) Yes <code>build_embeddings_for_memory</code> Enable embedding-based search Yes <p>Embedding Dimensions</p> <ul> <li>OpenAI <code>text-embedding-3-small</code>: 1536</li> <li>Google <code>text-embedding-004</code>: 768</li> <li>Match the dimension to your embedding model</li> </ul>"},{"location":"memory-write/configuration/#meta-agent-configuration","title":"Meta Agent Configuration","text":"Field Description <code>agents</code> List of memory agents to enable. Standard set includes all 8 agents. <code>memory.core</code> Initial core memory blocks (human info, persona) <code>memory.decay.fade_after_days</code> Days until memories become inactive (excluded from retrieval) <code>memory.decay.expire_after_days</code> Days until memories are permanently deleted <code>system_prompts_folder</code> Optional: custom system prompts directory"},{"location":"memory-write/configuration/#memory-agents","title":"Memory Agents","text":"<p>The standard agent configuration includes:</p> <ul> <li>core_memory_agent: Stores persistent facts about the user</li> <li>episodic_memory_agent: Records events and interactions</li> <li>semantic_memory_agent: Stores concepts and entities</li> <li>procedural_memory_agent: Remembers processes and workflows</li> <li>resource_memory_agent: Manages documents and files</li> <li>knowledge_memory_agent: Stores sensitive information (passwords, keys)</li> <li>reflexion_agent: Analyzes and improves memory quality</li> <li>background_agent: Handles asynchronous memory processing</li> </ul>"},{"location":"memory-write/configuration/#loading-configuration","title":"Loading Configuration","text":""},{"location":"memory-write/configuration/#from-yaml-file","title":"From YAML File","text":"<pre><code>import yaml\nfrom mirix import MirixClient\n\n# Load config from file\nwith open(\"mirix/configs/examples/mirix_openai.yaml\", \"r\") as f:\n    config = yaml.safe_load(f)\n\nclient = MirixClient(api_key=\"your_api_key_here\")\nclient.initialize_meta_agent(config=config)\n</code></pre>"},{"location":"memory-write/configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from mirix import MirixClient\n\nclient = MirixClient(api_key=\"your_api_key_here\")\n\nconfig = {\n    \"llm_config\": {\n        \"model\": \"gpt-4o-mini\",\n        \"model_endpoint_type\": \"openai\",\n        \"model_endpoint\": \"https://api.openai.com/v1\",\n        \"context_window\": 128000,\n    },\n    \"build_embeddings_for_memory\": True,\n    \"embedding_config\": {\n        \"embedding_model\": \"text-embedding-3-small\",\n        \"embedding_endpoint\": \"https://api.openai.com/v1\",\n        \"embedding_endpoint_type\": \"openai\",\n        \"embedding_dim\": 1536,\n    },\n    \"meta_agent_config\": {\n        \"agents\": [\n            \"core_memory_agent\",\n            \"resource_memory_agent\",\n            \"semantic_memory_agent\",\n            \"episodic_memory_agent\",\n            \"procedural_memory_agent\",\n            \"knowledge_memory_agent\",\n            \"reflexion_agent\",\n            \"background_agent\",\n        ],\n        \"memory\": {\n            \"core\": [\n                {\"label\": \"human\", \"value\": \"\"},\n                {\"label\": \"persona\", \"value\": \"I am a helpful assistant.\"},\n            ],\n            \"decay\": {\n                \"fade_after_days\": 30,\n                \"expire_after_days\": 90,\n            },\n        },\n    }\n}\n\nclient.initialize_meta_agent(config=config)\n</code></pre>"},{"location":"memory-write/configuration/#environment-variables","title":"Environment Variables","text":"<p>API keys can be set via environment variables instead of hardcoding them:</p> <pre><code># OpenAI\nexport OPENAI_API_KEY=sk-proj-xxx\n\n# Anthropic\nexport ANTHROPIC_API_KEY=sk-ant-xxx\n\n# Google Gemini\nexport GEMINI_API_KEY=your-gemini-key\n</code></pre> <p>Then omit the <code>api_key</code> field from your YAML configuration.</p>"},{"location":"memory-write/configuration/#configuration-examples","title":"Configuration Examples","text":"<p>All example configurations are available in the Mirix repository at <code>mirix/configs/examples/</code>:</p> <ul> <li><code>mirix_openai.yaml</code> - OpenAI GPT models</li> <li><code>mirix_gemini.yaml</code> - Google Gemini models</li> <li><code>mirix_claude.yaml</code> - Anthropic Claude models</li> <li><code>mirix_openai_single_agent.yaml</code> - Simplified single-agent setup</li> <li><code>mirix_gemini_single_agent.yaml</code> - Simplified Gemini setup</li> </ul> <p> Quickstart Next: Memory Search </p>"}]}